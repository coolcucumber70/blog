# 2022.8_3

## question

### 何为自博弈？

参考文章： https://zhuanlan.zhihu.com/p/388045867

### 关于自博弈的循环缺陷

### 关于序列模型

序列模型（Sequence Model）是一类机器学习模型，用于处理序列数据，其中输入和输出是按照时间顺序排列的。

序列模型广泛应用于自然语言处理（NLP）、语音识别、机器翻译、音乐生成等任务，因为这些任务的输入和输出通常是一个个时间步骤上的序列。

常见的序列模型包括：

隐马尔可夫模型（Hidden Markov Model，HMM）：HMM是一种生成模型，用于建模离散可观测序列和对应的隐藏状态序列之间的关系。它在语音识别、手写识别等任务中应用广泛。

递归神经网络（Recurrent Neural Network，RNN）：RNN是一种基于循环结构的神经网络，能够处理可变长度的序列数据。RNN通过在时间步骤之间传递隐藏状态来捕捉序列中的上下文信息，适用于序列生成、语言建模等任务。

长短时记忆网络（Long Short-Term Memory，LSTM）：LSTM是RNN的一种变体，通过引入门控机制解决了传统RNN中的梯度消失和梯度爆炸问题。LSTM在处理长序列和长期依赖关系时表现出色。

门控循环单元（Gated Recurrent Unit，GRU）：GRU是另一种RNN的变体，也具有门控机制，可以用于建模序列数据。相比LSTM，GRU具有更简化的结构。

转换器（Transformer）：Transformer是一种基于自注意力机制的序列模型，用于处理序列数据，如机器翻译和语言建模。它在NLP领域引入了注意力机制的革命性进展。

这些序列模型在处理序列数据时具有不同的特点和应用场景，选择适合特定任务的序列模型可以帮助提高模型性能和效果。

SM技术带来了Transformer等有效和表达性强的网络结构，在强化学习（RL）领域也吸引了极大的关注，这导致了一系列基于Transformer架构的成功离线RL算法的发展[5、14、30、23]。这些方法在解决一些最基本的RL训练问题，如长视野信用分配和奖励稀疏性[37、24、25]方面显示出了极大的潜力。

### 如何利用序列模型建模多智能体

我们首先提出了一种新的多智能体强化学习训练范式，该范式建立了合作多智能体强化学习问题和序列建模问题之间的联系。该范式的核心是**多智能体优势分解定理**和**顺序更新策略**，有效地将多智能体联合策略优化转化为顺序策略搜索过程。作为我们发现的自然结果，我们引入了Multi-Agent Transformer (MAT)，这是一种编码器-解码器架构，通过序列模型实现通用的多智能体强化学习解决方案。

### 何为蒙德卡洛树搜索

MCTS的主要概念还是搜索。搜索是沿着游戏树的一组遍历的集合，单次遍历是从根节点（当前游戏状态）到一个未完全展开节点的路径。一个未完全展开的节点意味着它至少有一个未被访问的子节点。当遇到未完全展开的节点时，从该节点的子节点中选取一个未被访问过的用来进行一次模拟。模拟的结果然后反向转播是当前树的根节点，并更新节点的统计信息。当搜索结束时（受限于时间或计算能力），就可以根据收集的统计信息来决定下一步怎么走。

参考文献：

https://blog.csdn.net/weixin_48866452/article/details/109449439?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-2-109449439-blog-81387170.235^v38^pc_relevant_anti_t3&spm=1001.2101.3001.4242.2&utm_relevant_index=5


### alphastar 训练过程

参考文献：https://www.leiphone.com/category/ai/aDDh5MOlOsU22WvK.html