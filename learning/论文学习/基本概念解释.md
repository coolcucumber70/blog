# 1

## MAPPO 和 HAPPO

MAPPO（Multi-Agent Proximal Policy Optimization）和HAPPO（Heterogeneous-Agent Proximal Policy Optimization）都是强化学习算法，用于解决多智能体协作问题。

MAPPO是一种基于邻近策略优化（Proximal Policy Optimization）的多智能体强化学习算法。它通过训练多个智能体来实现协同决策，每个智能体都有自己的策略网络。MAPPO使用近似策略梯度方法来更新智能体的策略，并通过共享参数和经验回放来提高训练效率和稳定性。MAPPO的目标是通过优化每个智能体的策略，使整个智能体群体能够实现更好的协同行为。

HAPPO是基于HARL（Heterogeneous-Agent Reinforcement Learning）框架的一种算法，其中提出了HAML（Heterogeneous-Agent Mirror Learning）框架。HAPPO是HAML框架的一种具体实现。HAPPO通过解决异构智能体协作中的问题，摆脱了参数共享的限制，提供了更好的稳定性和收敛保证。它利用多智能体优势分解引理和顺序更新方案来设计算法，从而实现了异构智能体的协同学习和决策。HAPPO是基于Proximal Policy Optimization（PPO）的一种算法变体，通过引入异构性来处理不同智能体之间的差异。

总的来说，MAPPO和HAPPO都是用于多智能体协作的强化学习算法，但HAPPO在处理异构智能体和解决参数共享问题方面提供了更具优势的方法。

## centralized training for decentralized execution

"centralized training for decentralized execution"（CTDE）是一种在多智能体强化学习中常见的训练方法。在这种方法中，智能体在训练过程中共享信息并进行集中化的训练，但在执行阶段，智能体根据各自的策略进行分散的决策和行动。

CTDE的目标是通过在训练过程中共享信息和进行集中化的训练来提高多智能体的协同效果和性能。在训练阶段，智能体可以共享彼此的经验和观察，并使用这些信息来改进自己的策略。这种集中化的训练方法可以更好地捕捉多智能体之间的相互影响和协作关系。

然而，在执行阶段，智能体通常需要根据各自的策略和局部观察做出决策，以实现分散的执行。这是因为在实际应用中，智能体可能分布在不同的位置或控制不同的操作，需要根据局部信息来进行决策。因此，尽管在训练过程中集中化，但在执行阶段智能体之间仍然是分散的。

CTDE方法可以在训练过程中有效地利用集中化的信息，提高多智能体系统的学习效率和性能。它适用于需要智能体之间协作和相互通信的任务，例如协同控制、博弈论等。

基于CTDE，由原本的单智能体算法像多智能体来引

### 1、COMA（Counterfactual Multi-Agent）

COMA（Counterfactual Multi-Agent）是一种用于多智能体强化学习的算法。COMA算法旨在解决多智能体协作问题中的策略优化和价值估计的挑战。

在传统的多智能体强化学习中，每个智能体基于自己的观察和奖励进行策略更新和值函数估计。然而，这种分离的方法无法有效地处理智能体之间的相互影响和协作关系。

COMA算法通过引入对抗性训练和反事实评估的思想来解决这个问题。它通过模拟不同的行动选择情景并对其进行反事实评估，来估计每个智能体的策略对全局收益的贡献。这样，COMA可以更准确地为每个智能体计算梯度，并优化它们的策略。

具体来说，COMA算法使用了一种叫做Centralized Q-Value Decomposition（CQVD）的方法，将全局状态和智能体的局部观察结合起来，计算每个智能体的局部价值函数。然后，通过引入Counterfactual Baseline和Advantage函数，COMA算法可以估计每个智能体的策略对全局收益的影响，并进行策略更新。

COMA算法在多智能体协作问题中取得了一定的成功，并被广泛用于解决具有多个协作智能体的任务，如多智能体控制、团队协作等。它能够有效地处理智能体之间的相互影响和协作关系，并提供了一种可靠的策略优化和价值估计方法。  

