#

## ABTRACT

GPT系列和BERT等大型序列模型（SM）在自然语言处理、视觉和最近的强化学习中表现出了出色的性能和泛化能力。 一个自然的后续问题是如何将多智能体决策也抽象为序列建模问题，并从智能体的繁荣发展中受益。 在本文中，我们介绍了一种名为多智能体变压器（MAT）的新颖架构，该架构有效地将协作多智能体强化学习（MARL）转化为 SM 问题，其中目标是将智能体的观察序列映射到智能体的最佳动作序列。 我们的目标是在 MARL 和 SM 之间架起桥梁，以便为 MARL 释放现代序列模型的建模能力。 我们的 MAT 的核心是编码器-解码器架构，它利用多智能体优势分解定理将联合策略搜索问题转化为顺序决策过程； 这使得多智能体问题的时间复杂度仅为线性，最重要的是，赋予 MAT 单调性能改进保证。 与 Decision Transformer 等现有技术仅适合预先收集的离线数据不同，MAT 是通过在线试验和环境中的错误以符合策略的方式进行训练的。 为了验证 MAT，我们对 StarCraftII、Multi-Agent MuJoCo、Dexterous Hands Manipulation 和 Google Research Football 基准进行了广泛的实验。 结果表明，与 MAPPO 和 HAPPO 等强大基线相比，MAT 实现了卓越的性能和数据效率。 此外，我们证明，无论智能体数量如何变化，MAT 在未见过的任务上都是一个出色的短时间学习器。 请参阅我们的项目页面：https://sites.google.com/view/multi-agent-transformer(1)。

## 1 Introduction


多智能体强化学习（MARL）[44, 8] 是一个具有挑战性的问题，因为它的难度不仅来自识别每个智能体的策略改进方向，还来自于将智能体的策略更新结合起来，这应该对整个团队有利 。 最近，由于引入了分散执行的集中训练（CTDE）[11, 45]，多智能体学习中的这种困难得到了缓解，它允许智能体在训练阶段访问全局信息和对手的行动。 该框架使得能够成功开发直接继承单代理算法的方法。 例如，COMA 用多智能体 PG (MAPG) 对应物替换策略梯度 (PG) 估计 [11]，MADDPG 将确定性策略梯度扩展到具有集中批评家的多智能体设置中 [20, 34]，QMIX 利用深度学习 用于分散代理的 Qnetworks 并引入了用于 Q 值分解的集中式混合网络 [29,36,26]。 MAPPO 赋予所有智能体相同的参数集，然后通过信任域方法进行训练[46]。 PR2[42]和GR2[43]方法在CTDE框架下进行递归推理。 然而，这些方法无法涵盖多智能体交互的全部复杂性； 事实上，其中一些在最简单的合作任务中都失败了[15]。 **为了解决这个问题，提出了多智能体优势分解定理[15，定理1]，它捕获了不同智能体如何对回报做出贡献，并通过顺序决策过程方案提供了合作出现背后的直觉。** 在此基础上，推导了HATRPO和HAPPO算法[15,17,16]，这些算法凭借**分解定理**和**顺序更新方案**，为MARL建立了新的最先进的方法。 然而，它们的局限性在于，代理人的政策不知道发展合作的目的，并且仍然依赖于精心设计的最大化目标。 理想情况下，代理团队应该通过设计意识到他们的培训的联合性，从而遵循整体且有效的范式——这是一个尚未提出的理想解决方案。

近年来，序列模型（SM）在自然语言处理（NLP）方面取得了实质性进展[27]。 例如，基于自回归 SM 构建的 GPT 系列 [3] 和 BERT 模型 [9]，在广泛的下游任务上表现出了卓越的性能，并在少样本泛化任务上取得了强劲的性能。 尽管 SM 由于其与语言的顺序特性自然契合而主要用于语言任务，但顺序方法不仅限于 NLP，而是一种广泛适用的通用基础模型 [2]。 例如，在计算机视觉（CV）中，人们可以将图像分割成子图像，并将它们按顺序对齐，就像它们是 NLP 任务中的标记一样 [9,10,12]。 尽管通过 SM 解决 CV 任务的想法很简单，但它是一些性能最佳的 CV 算法的基础 [38,41,39]。 此外，顺序方法最近开始产生强大的多模态视觉语言模型，例如 Flamingo [1]、DALL-E [28] 和 GATO [30]。

凭借 Transformer [40] 等有效且富有表现力的网络架构，序列建模技术也引起了 RL 社区的极大关注，这导致了一系列基于 Transformer 架构的成功的离线 RL 开发[5,14,30,23] ]。 这些方法在解决一些最基本的 RL 训练问题方面显示出巨大的潜力，例如长期信用分配和奖励稀疏性 [37,24,25]。 例如，通过以纯粹监督的方式在预先收集的离线数据上训练自回归模型，Decision Transformer [5] 绕过了通过动态编程计算累积奖励的需要，而是根据期望的回报、过去的状态和动作生成未来的动作 。 尽管取得了显着的成功，但这些方法都没有被设计来模拟多智能体系统中最困难的（也是 MARL 独有的）方面——智能体的交互。 事实上，如果我们简单地赋予所有智能体一个 Transformer 策略并独立训练它们，仍然不能保证它们的联合性能得到提高[15，命题 1]。 因此，虽然有无数强大的 SM 可供使用，但 MARL（一个将从 SM 中受益匪浅的领域）尚未真正利用它们的性能优势。 那么要问的关键研究问题是：

How can we model MARL problems by sequence models ?

在本文中，我们采取了几个步骤来对上述研究问题提供肯定的答案。 我们的目标是通过强大的序列建模技术增强 MARL 研究。 为了实现这一目标，我们首先提出一种新颖的 MARL 训练范式，该范式建立了协作 MARL 问题和序列建模问题之间的联系。 新范式的核心是多智能体优势分解定理和顺序更新方案，它们有效地将多智能体联合策略优化转化为顺序策略搜索过程。 作为我们研究结果的自然结果，我们引入了多代理变压器 (MAT)，这是一种通过 SM 实现通用 MARL 解决方案的编码器-解码器架构。 与 Decision Transformer [5] 不同，MAT 是基于策略上的试验和错误进行在线训练的； 因此，它不需要预先收集演示。 重要的是，多智能体优势分解定理的实现保证了MAT在训练过程中享有单调的性能提升保证。 MAT 为协作 MARL 任务建立了一个新的最先进的基线模型。 我们通过在 StarCraftII、Multi-Agent MuJoCo、Dexterous Hands Manipulation 和 Google Research Football 的基准上评估 MAT 来证明这一说法的合理性； 结果表明，MAT 的性能优于 MAPPO [46]、HAPPO [15]、QMIX [29] 和 UPDeT [13] 等强基线。 最后，我们表明 MAT 在任务泛化方面具有巨大的潜力，无论新任务中的智能体数量如何，该潜力都成立。

## 2 Preliminaries

在本节中，我们首先介绍协作 MARL 问题表述和多智能体优势分解定理，这是我们工作的基石。 然后我们回顾与 MAT 相关的现有 MARL 方法，最后让读者熟悉 Transformer。


当使用马尔可夫博弈对合作式多智能体强化学习问题进行建模时，表示为$\langle N, O, A, R, P, \gamma \rangle$ [19]。其中，$N = {1, \ldots , n}$ 是智能体的集合，$O = \prod_{i=1}^{n} O_i$ 是智能体的局部观察空间的乘积，即联合观察空间；$A = \prod_{i=1}^{n} A_i$ 是智能体的动作空间的乘积，即联合动作空间；$R : O \times A \rightarrow [-R_{\text{max}}, R_{\text{max}}]$ 是**联合奖励函数**；$P : O \times A \times O \rightarrow R$ 是转移概率函数；$\gamma \in [0, 1)$ 是折扣因子。在时间步$t \in N$，智能体$i \in N$ 观察到一个观察$o_{i,t} \in O_i$（$o = (o_1, \ldots , o_n)$ 是“联合”观察），并根据其策略$\pi_i$ 采取行动$a_{i,t}$，其中$\pi_i$ 是智能体的联合策略$\pi$ 的第$i$个分量。在每个时间步，所有智能体基于其观察同时采取行动，没有顺序依赖关系。转移核函数$P$和联合策略导致了（不完全的）边缘观察分布$\rho_\pi(\cdot)$，$\sum_{t=0}^{\infty} \gamma^t \text{Pr}(o_t = o|\pi)$。在每个时间步结束时，整个团队接收到一个联合奖励$R(o_t, a_t)$并观察到$o_{t+1}$，其概率分布是$P(\cdot|o_t, a_t)$。在无限长的过程中，智能体获得折扣累积回报$\sum_{t=0}^{\infty} \gamma^t R(o_t, a_t)$。

其中的公式如下所示：

智能体的联合观察空间：$O = \prod_{i=1}^{n} O_i$
智能体的联合动作空间：$A = \prod_{i=1}^{n} A_i$
联合奖励函数：$R : O \times A \rightarrow [-R_{\text{max}}, R_{\text{max}}]$
转移概率函数：$P : O \times A \times O \rightarrow R$
折扣因子：$\gamma \in [0, 1)$
智能体$i$的观察：$o_{i,t} \in O_i$
智能体$i$的策略：$\pi_i$
智能体的联合策略：$\pi = (\pi_1, \ldots , \pi_n)$
时间步$t$的智能体行动：$a_{i,t}$
智能体团队在时间步$t$的联合观察：$o = (o_1, \ldots , o_n)$
智能体团队在时间步$t$的联合奖励：$R(o_t, a_t)$
智能体团队观察到的下一个状态：$o_{t+1}$
$o_{t+1}$的概率分布：$P(\cdot|o_t, a_t)$
智能体累积折扣回报：$\sum_{t=0}^{\infty} \gamma^t R(o_t, a_t)$



单智能体马尔可夫决策过程：

