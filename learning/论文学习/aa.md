协作多智能体强化学习（MARL）旨在通过共享团队奖励来协调多个智能体的行动，适用于机器人群控制[9]、自主车辆协调[1]和网络路由[38]等众多任务。解决合作性 MARL 问题的自然方法是集中式方法，它将团队视为具有联合行动空间的单个参与者。尽管我们可以简单地将单智能体强化学习算法应用于此设置，但它通常无法很好地扩展，因为联合动作空间随着智能体数量呈指数增长[6,7,18]。此外，由于代理可观察性和通信的固有限制，它不适用于现实环境。
另一种方法是根据局部观察独立训练智能体来学习去中心化策略 [3, 27, 39]，但同时探索带来的非平稳性会导致不稳定的学习和收敛困难 [8, 40]。因此，大多数工作遵循集中式训练和去中心化执行（CTDE）范式[10, 17]，其中去中心化策略可以在训练期间访问额外的全局信息。 CTDE 范式的一个关键挑战是将环境的全局奖励正确地归因于智能体的个体行为，也称为信用分配问题 [2]。现有流行的 MARL 框架通常以一种难以解释的方式直接将全局 Q 值表示为每个智能体局部值的聚合 [13,20,25,37]。通过这种方式，这些隐式方法避免了显式协调分析，而是通过神经网络来拟合复杂的交互。尽管如果来自集中批评家的政策梯度携带足够的信息，信用分配可能不需要明确的公式[41]，但分散的参与者很难从隐含信息中提取直接且有价值的知识。如果没有明确评估每个代理的行为，他们就无法对每个代理的行为提供有价值的见解，并且缺乏策略优化的直接指导。此外，隐式方法经常面临表达能力的限制，因为它们经常对混合网络施加特定的约束[20, 25]。尽管以下一些工作[19, 22]试图解决这个问题，但它们经常引入额外的棘手计算，导致在复杂环境下表现平平。
为了解决这些问题，我们提出了一个顺序 MARL 评估框架来连续、明确地评估每个代理。该框架将对复杂相互作用的分析分解为顺序评估过程。在此分解中，我们通过按特定顺序评估代理来进行信用分配

其中每个智能体的评估基于其先前智能体的操作。我们在此框架下引入了顺序优势函数，以根据顺序优势函数明确地制定评估和优化智能体的策略。我们的顺序信用分配（SeCA）配备了序列调整算法，并根据每个智能体对团队的贡献动态学习评估序列。 SeCA避免了上述不明确和困难的学习，并通过为代理提供明确和直接的策略优化指导来追求高效的MARL。我们的可解释的信用分配体现在两个方面：1）SeCA 明确评估所有代理行为并阐明每个代理的行为，这与将团队奖励分解为个体效用作为隐式分析的常见做法不同，2）学习的评估序列是可解释的，这有助于代理有条不紊地协作（参见第 4.2 节的第四部分）。我们学习的直接性体现在直接促进政策学习的顺序优势的明确表述中。 SeCA 还比大多数价值分解方法实现了更高的表现力，因为我们的中心化批评家没有固有的限制。尽管一些工作[5, 29]也尝试给出明确的信用分配，但由于简单的实现或严格的限制，它们在复杂环境中往往表现不佳。 SeCA引入了更准确和更通用的评估公式，从而获得了更好的性能。我们总结我们的主要贡献如下：（1）我们提出了一个顺序的 MARL 评估框架，它将代理之间复杂的合作分析分解并简化为一系列可访问的评估。 (2)我们通过引入实现可解释的信用分配的序贯优势函数来制定序贯评估。此外，我们还提供了所提出的顺序优势方差的上限。 (3)我们提出了一种顺序调整算法来减轻评估顺序造成的影响。它利用集成梯度根据每个代理的贡献动态学习可解释的评估序列。凭借这些创新，SeCA 通过可解释的直接指导实现高效学习，并在一系列具有挑战性的《星际争霸 II》微观管理任务中取得有竞争力的表现 [21]。

合作MARL通过团队奖励来协调多个代理。促进协调的关键是将这种全局奖励正确分配给每个代理，称为信用分配问题。流行的隐性信用分配方法通常将团队奖励的价值分解为个人价值，缺乏对政策优化的可解释和直接的指导。早期的工作，VDN [25]，配备了线性分解并忽略了状态信息。 QMIX [20] 学习具有全局状态的非线性混合网络，并将各个状态动作值映射到联合 Q 值估计中。尽管QMIX在各种环境中表现良好，但仍然面临混合网络的单调性约束限制。 QTRAN [22] 通过使用各个效用和全局状态动作值之间的线性约束，进一步避免了这种表示限制。它保证了最佳的去中心化，但其约束在计算上很难处理，并且松弛常常导致性能不理想。 VMIX[23]将A2C与QMIX结合起来，将单调性约束扩展到价值网络，并用单调混合网络代替价值网络。 QPLEX [28]按照决斗结构分解Q值，将单调性条件从Q值转移到优势值。 QPD [36] 使用集成梯度归因来沿着轨迹路径分解团队奖励。然而，QPD 的个人奖励是否应该与代理人的贡献线性相关仍不清楚。基于策略的方法 LICA [41] 学习端到端可微策略优化以消除单调性约束。至于显式信用分配方法，一些尝试将全局奖励归因于遵循显式公式的个体行为。尽管显式方法揭示了哪些代理行为对团队奖励负责，但由于代理之间的交互非常复杂，现有的工作表现不佳。著名的 COMA [5] 利用反事实基线来计算优势函数。然而，它的偏见优势是根据其他智能体当前的行为来评估每个智能体的行为，并忽略它们的相互作用。 SQDDPG[29]和Shapley Counterfactual Credits（SCC）[11]通过Shapley Q值分配全局奖励，并通过网络或反事实方法反映每个代理的边际贡献。 SQDDPG提供了一个理论上合理的框架，但可观测性和凸博弈的假设限制了其应用范围。

3.1

多智能体系统中的相互作用是复杂的。每个智能体根据受其他智能体干扰的环境做出决策，所有智能体的行为共同决定了奖励。因此，明确评估每个agent的行为需要考虑其他agent的行为。当我们没有评估其他代理人时，很难确定代理人的行为对团队的影响。因此，我们的目标是提出一个顺序的过程来明确地评估每个代理的行为，并促进它们之间的合作。
本节介绍了一个顺序 MARL 评估框架，将复杂的交互分析分解为一系列可访问的评估。 我们的关键假设是团队中某些智能体的评估受其他智能体行为的影响较小。 例如，在评估公司员工的行动时，CEO的决定至关重要，因为我们要判断员工是否服从命令。 相反，员工的直觉行为对评估CEO的决策影响不大。 在评估首席执行官时，我们经常考虑外部因素，例如建模为状态信息的市场状况。 有了这种洞察力，在评估多智能体系统中每个智能体的行为时，我们可以首先评估受影响较小的智能体（例如本例中的首席执行官），然后根据这些已研究过的智能体的行为来分析其他智能体。 为了制定这个顺序评估过程，我们引入 Eval(A) 作为一组代理 A 的评估，以建模顺序 MARL 评估框架。 具体来说，具有 n 个智能体的多智能体系统 A 的合作研究可以分解为一系列子评估：



遵循 CTDE 范式，我们为每个代理网络使用一个集中批评家来遵循基于该批评家估计的优势函数 A 的梯度：

每个参与者的优势函数 A 明确地推断出该特定代理如何为团队做出贡献。 式（2）表明，优势值A直接决定了每次迭代时策略更新的规模。 不合理的优势值会导致振荡和拖延学习，甚至在简单的场景下也可能导致收敛到局部最优解。

著名的显式信用分配方法 COMA [5] 使用受差异奖励启发的反事实基线 [35]。 对于每个智能体 a，COMA 的反事实优势 Ac af 将动作 ua 的 Q 值与反事实基线进行比较，该基线仅边缘化 ua，同时保持 u−a 固定，即：

方程（3）的第二项表示反事实基线评估智能体 a 的动作，前提是其他智能体选择动作 u−a 。 它忽略了与 u−a′ , u−a 的潜在联合动作 (ua, u−a′) ，这可能会导致意想不到的结果。 因此，反事实优势仍然面临训练不稳定和低效率的问题。 为了更好地评估每个智能体 a，一个简单的做法是考虑所有可能的动作组合与 ua 的影响，计算其他智能体动作 u−a 的期望：
