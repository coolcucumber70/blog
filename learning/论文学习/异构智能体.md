# Heterogeneous-Agent Reinforcement Learning

## Abstract

合作多智能体强化学习（MARL）在人工智能研究中日益受到重视，因为智能机器之间的合作变得越来越必要。然而，许多研究工作在智能体之间 **heavily rely on parameter sharing（共享参数）**，这限制了它们只能在同质智能体设置下运行，并导致训练的不稳定性和缺乏收敛保证。为了在一般的异构智能体设置下实现有效的合作，我们提出了解决上述问题的异构智能体强化学习（HARL）算法。我们的研究成果的核心是多智能体优势分解引理和顺序更新方案。基于这些，我们开发了经过证明正确的**异构智能体信任区域学习（HATRL）算法**，它摆脱了参数共享的限制，并通过可行的近似推导出了  。此外，我们发现了一种称为异构智能体镜像学习（HAML）的新框架，它加强了HATRPO和HAPPO的理论保证，并为合作MARL算法设计提供了一个通用模板。我们证明从HAML导出的所有算法本质上都享有联合奖励的单调改进和收敛到纳什均衡。作为其自然结果，HAML验证了更多的新算法，除了HATRPO和HAPPO，还包括HAA2C、HADDPG和HATD3，这些算法在性能上一致优于现有的多智能体对应算法。我们在六个具有挑战性的基准测试中全面测试了HARL算法，并证明它们相对于强基线（如MAPPO和QMIX）在协调异构智能体方面具有更高的效果和稳定性。

## 1 Introduction



协作多智能体强化学习（MARL）是多智能体系统中学习的自然模型，例如机器人群（Hüttenrauch et al., 2017）、自动驾驶汽车（Cao et al., 2012）和交通信号控制（ 卡尔沃和杜斯帕里克，2018）。 为了解决协作 MARL 问题，一种简单的方法是直接将单智能体强化学习算法应用于每个智能体，并将其他智能体视为环境的一部分，这种范式通常称为独立学习(**Independent Learning**)（de Witt et al., 2020） 。

 尽管在某些任务中有效，但独立学习在面对更复杂的场景时会失败（Hu et al., 2022; Foerster et al., 2018），直观上很清楚：一旦学习代理更新其策略，它的队友也会更新， 这会导致每个代理的有效环境发生变化，这是单代理算法无法应对的（Claus 和 Bouutilier，1998）。 为了解决这个问题，开发了一种名为集中训练与分散执行**Centralised Training with Decentralised Execution**（CTDE）的学习范式（Lowe 等人，2017b；Foerster 等人，2018；Zhou 等人，2021）。 在 CTDE 中，每个智能体都配备了一个联合价值函数，在训练过程中，该函数可以访问全局状态和队友的行动。 在考虑其他因素造成的非平稳性的集中价值函数的帮助下，每个代理相应地调整其策略参数。 因此，它有效地利用了全球信息，同时仍然保留了分散的执行代理。 因此，CTDE 范式允许将单智能体策略梯度定理（Sutton 等人，2000；Silver 等人，2014）直接扩展到多智能体场景（Lowe 等人，2017b；Kuba 等人， 2021 年；Mguni 等人，2021 年）。 因此，人们开发出了卓有成效的多智能体策略梯度算法（Foerster et al., 2018；Peng et al., 2017；Zhang et al., 2020；Wen et al., 2018, 2020；Yang et al., 2018）。 然而，由于多种原因，现有方法并不是最终的解决方案。 首先，一些算法（Yu et al., 2021; de Witt et al., 2020）依赖于参数共享，并要求代理是同质的（即共享相同的观察空间和动作空间，并在合作任务中扮演相似的角色） ），这在很大程度上限制了它们的适用性并可能损害性能。正如我们稍后在第 2.3.1 节中所展示的，参数共享可能会出现指数次优**exponentially-suboptimal outcome**的结果。 其次，现有算法同时更新代理。 由于他们采用的多智能体策略梯度估计器的高方差（Kuba et al., 2021），这种特定的更新方案通常会导致训练不稳定和收敛失败。 最后，一些算法，例如IPPO和MAPPO，是基于直觉和经验实验开发的，而不是以理论推导为基础的。 缺乏理论会损害其重要用途的可信度。
 
为 了解决这些问题，在这项工作中，我们提出了**Heterogeneous-Agent Reinforcement Learning**异构智能体强化学习（HARL）算法系列，该算法系列适用于一般的异构智能体设置（即，对观察空间、动作空间和智能体角色没有限制） ，通过新颖的顺序更新方案实现有效协调，并且有理论依据。
 特别是，**我们利用多智能体优势分解引理（Kuba et al., 2021）并推导了信任域学习的理论上支持的多智能体扩展，该扩展被证明具有单调改进特性并收敛于纳什均衡（ NE）保证**。 基于此，我们提出**Heterogeneous-Agent Trust Region Policy Optimisation**异构代理信任域策略优化（HATRPO）算法和**Heterogeneous-Agent Proximal Policy Optimisation**异构代理近端策略优化（HAPPO）算法作为理论过程的易于处理的近似。 
 
 此外，受镜像学习（Kuba et al., 2022）的启发，镜像学习为 TRPO 和 PPO 的有效性提供了理论解释，我们发现了一种名为异构代理镜像学习（HAML）的新颖框架，它加强了 HATRPO 和 HAPPO 的理论保证 并为协作式 MARL 算法设计提供了通用模板。 我们证明，所有源自 HAML 的算法本质上都满足联合奖励单调改进和收敛到纳什均衡的期望属性。 因此，HAML 极大地扩展了理论上合理的算法空间，并有可能为更实际的设置提供协作的 MARL 解决方案。 我们探索 HAML 类并推导出更有理论基础和实用的异构代理算法，包括 HAA2C、HADDPG 和 HATD3。 

为了方便 HARL 算法的使用，我们开源了基于 PyTorch 的集成实现。 在此基础上，我们在多智能体粒子环境（MPE）（Lowe et al., 2017a; Mordatch and Abbeel, 2017）、多智能体MuJoCo（MAMuJoCo）（Peng et al., 2021）、星际争霸上全面测试HARL算法 多智能体挑战 (SMAC)（Samvelyan 等人，2019）、SMACv2（Ellis 等人，2022）、Google 研究足球环境（GRF）（Kurach 等人，2020）和 Bi-DexterousHands（Chen 等人） ., 2022)。 实证结果证实了算法在实践中的有效性。 在包括 MPE、MAMuJoCo、GRF 和 Bi-Dexteroushands 在内的异构智能体的所有基准测试中，HARL 算法始终优于现有的 MA 对应模型，并且随着智能体异构性的增加，它们的性能差距变得更大，这表明 HARL 算法更加稳健和更好 适合一般的异构代理设置。 虽然所有 HARL 算法都显示出具有竞争力的性能，但它们在 HAPPO 和 HATD3 中达到了顶峰，它们建立了新的最先进结果。 作为一种离策略算法，HATD3 还提高了样本效率，从而实现更高效的学习和更快的收敛。 在代理大多同质的任务（例如 SMAC 和 SMACv2）上，HAPPO 和 HATRPO 在收敛时获得了相当或更高的获胜率，而不依赖于限制性参数共享技巧，这证明了它们的普遍适用性。 通过消融分析，我们凭经验证明了 HARL 理论和算法引入的新颖性对于学习最优合作策略至关重要，从而表明了它们的重要性。

## Preliminaries

我们考虑一个完全合作的多智能体任务，可以描述为马尔可夫博弈（MG）（Littman，1994），也称为随机博弈（Shapley，1953），正式定义如下:

Definition 1 A cooperative Markov game is defined by a tuple 〈N , S, A, r, P, γ, d〉. Here, N = {1, . . . , n} is a set of n agents, S is the state space, A = ×in=1Ai is the products of all agents’ action spaces, known as the joint action space. Further, r : S × A → R is the joint reward function, P : S × A × S → [0, 1] is the transition probability kernel, γ ∈ [0, 1) is the discount factor, and d ∈ P(S) (where P(X) denotes the set of probability distributions over a set X) is the positive initial state distribution.

智能体的动作空间（Action Space）：每个智能体在特定时间点可以采取的可能动作的集合。

联合动作空间（Joint Action Space）：所有智能体的动作空间的笛卡尔积，即所有智能体可能的联合动作的集合。

联合奖励函数（Joint Reward Function）：一个函数，将当前状态（State）和联合动作映射到一个实数奖励（Reward）。这个函数用于评估联合动作的好坏。

转移概率核函数（Transition Probability Kernel）：一个函数，描述在给定当前状态和联合动作的情况下，下一个状态发生的概率分布。

折扣因子（Discount Factor）：一个介于0和1之间的值，用于衡量未来奖励的重要性。较大的折扣因子表示更关注未来的奖励，而较小的折扣因子更关注即时奖励。

初始状态分布（Initial State Distribution）：一个正的概率分布，表示在开始执行任务时，智能体可能处于的初始状态的分布。


这个公式可以解释为：

$$\Pi^{i}\triangleq{\times_{s\in\mathcal{S}}\pi^{i}(\cdot^{i}|s)|\forall s\in\mathcal{S},\pi^{i}(\cdot^{i}|s)\in\mathcal{P}(\hat{\mathcal{A}^{i}})}$$

在公式中，我们定义了一个集合 $\Pi^{i}$，表示变量 i 的策略空间。

具体来说，公式中的符号和定义如下：

$\Pi^{i}$：表示变量 i 的策略空间。
$\times_{s\in\mathcal{S}}$：表示对状态空间 $\mathcal{S}$ 中的每个状态 s 进行操作。
$\pi^{i}(\cdot^{i}|s)$：表示在给定状态 s 的情况下，变量 i 的条件策略。它是一个函数，将变量 i 的取值映射到概率分布上。这里的 $\cdot^{i} $表示变量 i 的取值。
$\mathcal{P}(\hat{\mathcal{A}^{i}})$：表示变量 i 的动作空间 $\hat{\mathcal{A}^{i}}$ 中的概率分布集合。
因此，公式可以解释为，在策略空间 $\Pi^{i}$ 中，每个策略都由以下条件确定：对于状态空间 $\mathcal{S}$ 中的每个状态 s，变量 i 的条件策略 $\pi^{i}(\cdot^{i}|s)$ 是一个属于变量 i 的动作空间 $\hat{\mathcal{A}^{i}}$ 的概率分布。

这样，$\Pi^{i}$ 表示了变量 i 的策略空间，其中的每个策略都满足对应的条件，即在每个状态 s 下都属于变量 i 的动作空间 $\hat{\mathcal{A}^{i}}$的概率分布集合。

在公式 $\pi^{i}(\cdot^{i}|s)$ 中，$\pi^{i}(\cdot^{i}|s)$ 是一个条件策略函数，用于表示在给定状态 $s$ 的情况下，变量 $i$ 的策略。


$\pi^{i}(\cdot^{i}|s)$：表示在给定状态 $s$ 的情况下，代理 $i$ 的策略。它是一个函数，将代理 $i$ 的行动映射到概率分布上。
当 $\pi^{i}(\cdot^{i}|s)$ 是一个狄拉克（Dirac）δ分布时，对于所有的状态 $s \in S$，该策略被称为确定性策略（deterministic policy）。在这种情况下，我们用 $\mu^{i}(s)$ 表示其中心值。

接下来，环境将产生联合奖励 $r_t = r(s_t, a_t)$ 并转移到下一个状态 $s_{t+1} \sim P(\cdot|s_t, a_t) \in P(S)$。联合策略 $\pi$、转移概率核函数 $P$ 和初始状态分布 $d$ 会导致在时间 $t$ 的边际状态分布，记为 $\rho_{t}^{\pi}$。我们定义一个（不完全的）边际状态分布 $\rho^{\pi} = \sum_{t=0}^{\infty} \gamma^t \rho_{t}^{\pi}$，其中 $\gamma$ 是折扣因子。

状态值函数（state value function）和状态行动值函数（state-action value function）的定义如下：



状态值函数 $V^{\pi}(s)$：在策略 $\pi$ 下，从状态 $s$ 开始的期望累积奖励。它表示在策略 $\pi$ 下，从状态 $s$ 开始，预期获得的未来奖励。
状态行动值函数 $Q^{\pi}(s, a)$：在策略 $\pi$ 下，从状态 $s$ 开始，采取行动 $a$ 后的期望累积奖励。它表示在策略 $\pi$ 下，从状态 $s$ 开始，采取行动 $a$，然后按照策略 $\pi$ 继续执行，预期获得的未来奖励。


$$V_\pi(s)\triangleq\mathbb{E}_{\mathbf{a}_{0:\infty}\sim\mathbf{\pi},\mathbf{s}_{1:\infty}\sim P}\big[\sum_{t=0}^\infty\gamma^t\text{r}_t\big|\text{s}_0=s\big]$$

$$Q_\pi(s,\boldsymbol{a})\triangleq\mathbb{E}_{\mathbf{s}_{1:\infty}\sim P,\mathbf{a}_{1:\infty}\sim\boldsymbol{\pi}}[\sum_{t=0}^\infty\gamma^t\text{r}_t|\mathrm{~s}_0=s,\mathrm{~a}_0=\boldsymbol{a}]$$
在本文中，我们考虑完全合作的环境，其中代理的目标是最大化预期联合回报，定义为:
$$J(\pi)\triangleq\mathbb{E}_{\mathrm{s}_{0:\infty}\sim\rho_\pi^{0:\infty},\mathbf{a}_{0:\infty}\sim\pi}\left[\sum_{t=0}^\infty\gamma^t\mathrm{r}_t\right]$$

**定义2**： 在一个完全合作的游戏中，如果对于每个玩家 i ∈ N，当 πi ∈ Πi 时，满足 J (π∗) ≥ J (πi, π−i ∗)，那么一个联合策略 π∗ = (π1 ∗, . . . , πn ∗ ) 就是一个纳什均衡（NE）


这个定义表明，在完全合作的游戏中，如果联合策略使得整体的收益 J (π∗) 不小于任何一个单独玩家采取策略 πi 时的收益 J (πi, π−i ∗)，那么这个联合策略就被认为是一个纳什均衡。也就是说，没有任何一个玩家能通过单独改变自己的策略来提高整体的收益。

NE是一个已经确立的博弈论解决方案概念。第二个定义描述了合作多智能体强化学习任务中收敛时的均衡点。为了研究找到纳什均衡的问题，我们密切关注不同智能体子集对性能的贡献。为此，我们引入以下新的定义。

**定义3**: 当 m = 1 时，我们用 $i$ 表示，表示为 N 的一个有序子集 {i}。对应地，我们用 $-i$ 表示其补集。而当 m > 1 时，我们用 $i_{1:m}$ 表示 N 的一个有序子集 {i_1, ..., i_m}，用 $-i_{1:m}$ 表示其补集。

在此基础上，我们用 $i_k$ 来表示有序子集中的第 k 个智能体。多智能体状态-动作值函数的定义如下：
$$Q_\pi^{i_{1:m}}\left(s,a^{i_{1:m}}\right)\triangleq\mathbb{E}_{\mathbf{a}^{-i_{1:m}\sim\pi^{-i_{1:m}}}}\left[Q_\pi\left(s,a^{i_{1:m}},\mathbf{a}^{-i_{1:m}}\right)\right]$$

这个式子表示多智能体状态-动作值函数 $Q_\pi^{i_{1:m}}\left(s,a^{i_{1:m}}\right)$ 的定义。让我们逐步解释：

$Q_\pi^{i_{1:m}}\left(s,a^{i_{1:m}}\right)$：表示在给定策略 $\pi$ 下，对于有序子集 $i_{1:m}$ 中的智能体所采取的动作 $a^{i_{1:m}}$ 在状态 $s$ 下的状态-动作值函数。

$\mathbb{E}{\mathbf{a}^{-i{1:m}}\sim\pi^{-i_{1:m}}}$：表示对于 $i_{1:m}$ 之外的智能体（补集 $-i_{1:m}$）所采取的动作 $\mathbf{a}^{-i_{1:m}}$ 进行期望计算。

$Q_\pi\left(s,a^{i_{1:m}},\mathbf{a}^{-i_{1:m}}\right)$：表示在给定策略 $\pi$ 下，所有智能体采取动作 $a^{i_{1:m}}$ 和 $\mathbf{a}^{-i_{1:m}}$ 的情况下的状态-动作值函数。

换句话说，$Q_\pi^{i_{1:m}}\left(s,a^{i_{1:m}}\right)$ 表示在给定策略 $\pi$ 下，对于有序子集 $i_{1:m}$ 中的智能体所采取的动作 $a^{i_{1:m}}$，考虑其他智能体（补集 $-i_{1:m}$）按照策略 $\pi^{-i_{1:m}}$ 采取动作的情况下的期望状态-动作值函数。

这个定义用于描述多智能体系统中，特定子集的智能体在给定策略下的状态-动作值函数，并考虑了其他智能体的策略对其的影响。

具体而言，当 m = n（考虑所有智能体的联合动作）时，i1:n ∈ Sym(n)，其中Sym(n)表示整数 1, . . . , n 的排列集合，也称为对称群。在这种情况下，$Q_{i1:n}^{\pi}(s, a_{i1:n})$ 等价于 $Q^{\pi}(s, a)$。另一方面，当 m = 0，即 i1:m = ∅ 时，函数的形式就变成了 $V^{\pi}(s)$。此外，考虑两个不相交的智能体子集，j1:k 和 i1:m。那么，对于 i1:m 来说，相对于 j1:k 的多智能体优势函数被定义为：

$$A_\pi^{i_{1:m}}\left(s,\boldsymbol{a}^{j_{1:k}},\boldsymbol{a}^{i_{1:m}}\right)\triangleq Q_\pi^{j_{1:k},i_{1:m}}\left(s,\boldsymbol{a}^{j_{1:k}},\boldsymbol{a}^{i_{1:m}}\right)-Q_\pi^{j_{1:k}}\left(s,\boldsymbol{a}^{j_{1:k}}\right)$$

$A_\pi^{i_{1:m}}\left(s,\boldsymbol{a}^{j_{1:k}},\boldsymbol{a}^{i_{1:m}}\right)$ 表示在给定策略 $\pi$ 下，对于智能体子集 $i_{1:m}$ 采取动作 $\boldsymbol{a}^{i_{1:m}}$，对于智能体子集 $j_{1:k}$ 采取动作 $\boldsymbol{a}^{j_{1:k}}$ 的多智能体优势函数。

$Q_\pi^{j_{1:k},i_{1:m}}\left(s,\boldsymbol{a}^{j_{1:k}},\boldsymbol{a}^{i_{1:m}}\right)$ 表示在给定策略 $\pi$ 下，智能体子集 $j_{1:k}$ 和 $i_{1:m}$ 分别采取动作 $\boldsymbol{a}^{j_{1:k}}$ 和 $\boldsymbol{a}^{i_{1:m}}$ 的状态-动作值函数。

$Q_\pi^{j_{1:k}}\left(s,\boldsymbol{a}^{j_{1:k}}\right)$ 表示在给定策略 $\pi$ 下，智能体子集 $j_{1:k}$ 采取动作 $\boldsymbol{a}^{j_{1:k}}$ 的状态-动作值函数。

这个定义描述了对于智能体子集 $i_{1:m}$，相对于智能体子集 $j_{1:k}$ 的多智能体优势函数。它衡量了在给定策略下，$i_{1:m}$ 采取动作和 $j_{1:k}$ 采取动作之间的差异。 

*

，$Q_{i1:m}^\pi(s, a_{i1:m})$ 评估智能体子集 $i_{1:m}$ 在状态 $s$ 下采取动作 $a_{i1:m}$ 的值，同时对 $a_{-i1:m}$ 进行边际化处理。而 $A_{i1:m}^\pi(s, a_{j1:k}, a_{i1:m})$ 评估智能体子集 $i_{1:m}$ 在状态 $s$ 下采取动作 $a_{i1:m}$ 的优势，假设智能体子集 $j_{1:k}$ 采取动作 $a_{j1:k}$，其余智能体的动作通过期望进行边际化处理。正如我们后面在第3节中展示的，这些函数允许分解联合优势函数，从而为信用分配问题提供新的视角。


### 2.2 Dealing With Partial Observability

值得注意的是，在某些合作多智能体任务中，全局状态 $s$ 可能对智能体来说是部分可观测的。换句话说，每个智能体只能感知到环境的局部观测，而不满足马尔可夫性质。解决部分可观测性的模型称为分散部分可观测马尔可夫决策过程（Dec-POMDP）（Oliehoek和Amato，2016）。然而，Dec-POMDP 被证明是 NEXP-完全的（Bernstein等，2002），在最坏情况下解决它需要超指数时间（Zhang等，2021）。为了得到可行的结果，在理论推导中我们假设完全可观测性，并让每个智能体在全局状态的条件下采取动作，即 $a_{it} \sim \pi_i(\cdot|i,s)$，(补：$a_{it} \sim \pi_i(\cdot|i,s)$ 表示智能体 $i$ 在给定全局状态 $s$ 和自身观测 $i$ 的条件下，按照策略 $\pi_i$ 采取动作 $a_{it}$。换句话说，智能体 $i$ 的动作 $a_{it}$ 是根据其自身的策略 $\pi_i$ 在给定的全局状态 $s$ 和自身观测 $i$ 的条件下随机选择的。这个表示方式表明智能体在决策时依赖于自身的观测和全局状态，并按照其策略进行动作选择。)从而得到实际的算法。在我们的实现中，我们通过使用循环神经网络（RNN）来弥补部分可观测性，使得智能体的动作可以依赖于动作-观测历史，或者直接使用多层感知机（MLP）网络，使智能体的动作依赖于部分观测。这两种方法都是现有工作中常见的方法，包括 MAPPO（Yu等，2021）、QMIX（Rashid等，2018）、COMA（Foerster等，2018）等。通过我们的实验（第5节），我们展示了这两种方法都足以解决部分可观测任务。

### 2.3 The State of Affairs in Cooperative MARL

在我们回顾用于合作多智能体强化学习（MARL）的现有最先进算法之前，我们介绍两种可供实现这些算法的设置。这两种设置都具有吸引力，具体取决于应用场景，但它们的优势也伴随着一些“陷阱”，如果不处理好，可能会明显降低算法的性能。

#### 2.3.1 Homogeneity vs. Heterogeneity

*（在合作多智能体强化学习（MARL）中，智能体可以根据其同质性或异质性分为两类*。

*同质性指的是所有智能体具有相同的能力、目标和行为的设置。在这种情况下，智能体是相同且不可区分的。它们共享相同的策略并协同学习以实现共同的目标。同质性设置简化了学习过程，因为所有智能体可以遵循相同的算法，并以同步的方式更新其策略。它促进了智能体之间的协调与合作，从而实现高效且有效的集体决策。*

*另一方面，异质性涉及具有不同能力、目标或行为的智能体。每个智能体可能具有不同的观测空间、动作空间或目标。异质性设置带来了额外的挑战，因为智能体之间可能存在利益冲突或相互了解有限的情况。智能体需要调整其策略和策略来考虑这些差异，并有效地协调其行动。异质性设置需要更复杂的算法，可以处理多样性，并使智能体在团队中专注于不同的任务或角色。）*

第一种设置是同质策略（homogeneous policies），即所有智能体共享一个策略：$\pi_i = \pi, \forall i \in N$，因此$\pi = (\pi, \ldots, \pi)$（de Witt et al., 2020；Yu et al., 2021）。这种方法可以直接采用强化学习算法应用于合作多智能体强化学习（MARL），并且随着智能体数量的增加，不会引入太多的计算和样本复杂度负担。然而，共享一个策略要求智能体的动作空间也是相同的，即$A_i = A_j, \forall i, j \in N$。此外，策略共享会阻止智能体学习多样化的联合策略。在智能体数量较多的游戏中，这种情况可能特别危险，详见附录A中证明的命题4。

命题4（同质性的陷阱）考虑一个完全合作的游戏，有偶数个智能体$n$，一个状态，和联合动作空间$\{0, 1\}^{n}$，其中奖励由$r(0^{n/2}, 1^{n/2}) = r(1^{n/2}, 0^{n/2}) = 1$给出，并且对于所有其他联合动作 $r(a^{1:n}) = 0$ 。
设$J^*$为最优联合奖励，$J^*_{\text{share}}$为在共享策略约束下的最优联合奖励。那么：
$$\frac{J_{share}^*}{J^*}=\frac2{2^n}$$
。

*根据命题4，当智能体采用共享策略时，其最优联合奖励$J^*_{\text{share}}$与没有共享策略限制下的最优联合奖励$J^*$之间的比值为$\frac{2}{2^n}$。这意味着在共享策略的约束下，智能体能够获得的最优联合奖励是没有共享策略限制下最优联合奖励的$\frac{2}{2^n}$倍。这个比例随着智能体数量$n$的增加而减小，表示随着智能体数量的增加，共享策略的限制对最优联合奖励的影响逐渐减弱。*

一个更雄心勃勃的多智能体强化学习（MARL）方法是允许智能体之间的策略异质性，即当$i \neq j \in N$时，让$\pi_i$和$\pi_j$成为不同的函数。这种设置具有更大的适用性，因为异质性智能体可以在不同的动作空间中操作。此外，由于这种模型的灵活性，它们可以学习更复杂的联合行为。最后，如果确实是最优的，它们可以通过训练获得同质性策略。然而，训练异质性智能体是非常复杂的。给定一个联合奖励，个体智能体可能无法准确确定自己对奖励的贡献，这个问题被称为信用分配（Foerster等人，2018；Kuba等人，2021）。此外，即使一个智能体确定了自己的改进方向，它的改进方向可能与其他智能体的方向冲突，这将导致绩效损害，这在我们在附录中证明的命题5中进行了举例。

换句话说，允许智能体之间的策略异质性可以使系统更加灵活和适应不同的情境。不同的智能体可以学习不同的策略，以适应其个体特点和环境要求。然而，这也带来了一些挑战，包括信用分配问题和智能体之间的冲突。因此，在训练异质性智能体时需要仔细考虑这些问题，并采取相应的方法来解决它们。
**Proposition 5 (Trap of Heterogeneity)**
  

#### 2.3.2 Analysis of Existing Work

MAA2C（Papoudakis等人，2021）将A2C（Mnih等人，2016）算法扩展到多智能体强化学习（MARL），通过将强化学习优化（单智能体策略）目标替换为多智能体强化学习目标（联合策略）。


## 3 Our Methods


本节的目的是介绍异构代理强化学习（Heterogeneous-Agent Reinforcement Learning，HARL）算法系列，我们在理论上证明了它们可以解决合作问题。HARL算法旨在设计适用于异构代理的广泛和表达性设置，从而避免了命题4，并且它们的本质是协调代理的更新，摒弃了命题5。我们首先在第3.1节中开发了一个理论上合理的异构代理信任区域学习（Heterogeneous-Agent Trust Region Learning，HATRL）过程，并在第3.2节中导出了实际算法，即HATRPO和HAPPO，作为其可行的近似方法。我们进一步在第3.3节中引入了新颖的异构代理镜像学习（Heterogeneous-Agent Mirror Learning，HAML）框架，它增强了HATRPO和HAPPO的性能保证（第3.4节），并为合作多智能体强化学习算法的设计提供了通用模板，从而产生更多的HARL算法（第3.5节）。

### 3.1 Heterogeneous-Agent Trust Region Learning (HATRL)

根据直觉，如果我们分别对所有代理进行参数化，并让它们逐个学习，那么我们将打破同质性约束，并允许代理协调它们的更新，从而避免第2.3节中的两个陷阱。这种协调可以通过在当前代理的优化目标中考虑先前代理的更新来实现，沿着上述序列进行。幸运的是，这个想法体现在多代理优势函数$A_{im}^{\pi}(s, a_{i1:m-1}, a_{im})$中，该函数允许代理$im$在给定先前代理$ai1:m-1$的动作之后评估其动作$aim$的效用。有趣的是，多代理优势函数允许对联合优势函数进行严格的分解，如下关键引理所描述。

引理6（**多智能体优势函数分解**）在任何合作马尔可夫博弈中，给定一个联合策略$\pi$，对于任意状态$s$和任意代理子集$i1:m$，下面的方程成立：
$$
A_{i1:m}^{\pi}(s, a_{i1:m}) = \sum_{j=1}^m A_{ij}^{\pi}(s, a_{i1:j-1}, a_{ij}) $$
值得注意的是，引理6通常适用于合作马尔可夫博弈，不需要对联合值函数的可分解性作出任何假设，例如VDN（Sunehag等，2018）、QMIX（Rashid等，2018）或Q-DPP（Yang等，2020）中的假设。引理6确认了**顺序更新**是多智能体学习中寻找性能改进方向（即具有正优势值的联合动作）的有效方法。也就是说，想象一下代理按照任意顺序$i1:n$依次采取行动。让代理$i1$采取动作$\bar{a}_{i1}$，使得$A_{i1}(s, \bar{a}_{i1}) > 0$，然后对于剩余的$m = 2，...，n$，每个代理$i_m$采取动作$\bar{a}_{im}$，使得$A_{im}(s, \bar{a}_{i1:m-1}, \bar{a}_{im}) > 0$。根据引理6，对于诱导的联合动作$\bar{a}$，保证$A^{\pi_{\theta}}(s, \bar{a})$是正的，因此性能得到保证改善。为了将上述过程正式扩展为具有单调改进保证的策略迭代过程，我们首先介绍以下定义。

定义7：设π是一个联合策略，$ \bar{\pi}_{i1:m-1} = \prod_{j=1}^{m-1} \bar{\pi}_{ij} $ 是代理子集$i1:m-1$的另一个联合策略，$ \hat{\pi}_{im} $ 是代理$im$的另一个策略。那么，$ L_{i1:m}^{\pi}(\bar{\pi}_{i1:m-1}, \hat{\pi}_{im}) = \mathbb{E}_{s\sim\rho^{\pi}, a_{i1:m-1}\sim\bar{\pi}_{i1:m-1}, a_{im}\sim\hat{\pi}_{im}}[A_{im}^{\pi}(s, a_{i1:m-1}, a_{im})] $。

注意，对于任何$ \bar{\pi}_{i1:m-1} $，我们有$ L_{i1:m}^{\pi}(\bar{\pi}_{i1:m-1}, \pi_{im}) = \mathbb{E}_{s\sim\rho^{\pi}, a_{i1:m-1}\sim\bar{\pi}_{i1:m-1}, a_{im}\sim\pi_{im}}[A_{im}^{\pi}(s, a_{i1:m-1}, a_{im})] = \mathbb{E}_{s\sim\rho^{\pi}, a_{i1:m-1}\sim\bar{\pi}_{i1:m-1}}[\mathbb{E}_{a_{im}\sim\pi_{im}}[A_{im}^{\pi}(s, a_{i1:m-1}, a_{im})]] = 0$。

结合引理6和定义7，我们得到了联合策略更新的下界。

引理8：设π是一个联合策略。对于任何联合策略$ \bar{\pi} $，我们有$ J(\bar{\pi}) \geq J(\pi) + \sum_{m=1}^{n} [L_{i1:m}^{\pi}(\bar{\pi}_{i1:m-1}, \bar{\pi}_{im}) - C D_{\text{max}} \text{KL}(\pi_{im}, \bar{\pi}_{im})] $，其中$ C = 4\gamma \max_{s,a} |A^{\pi}(s, a)| (1 - \gamma)^2 $。

这个引理提供了如何改进联合策略的思路。根据方程（5），我们知道如果任何代理按顺序更新它们的策略，通过设置上述各项的值$ L_{i1:m}^{\pi}(\bar{\pi}_{i1:m-1}, \bar{\pi}_{im}) - C D_{\text{max}} \text{KL}(\pi_{im}, \bar{\pi}_{im}) $，每个代理都可以通过不进行策略更新（即$ \bar{\pi}_{im} = \pi_{im} $）使其各项为零。这意味着任何正的更新将导致总和的增加。此外，由于有n个代理进行策略更新，复合增量可能很大，从而实现实质性改进。最后，注意这个性质不要求代理按特定顺序进行更新，这允许在每次迭代中对更新顺序进行灵活调度。总结起来，我们提出以下算法1。

算法1如下所示：

初始化联合策略 $ \pi^0 = (\pi_1^0, ..., \pi_n^0) $。
对于 $ k = 0, 1, ... $，进行以下步骤：
1. 计算所有状态-（联合）动作对 $ (s, a) $ 的优势函数 $ A^{\pi^k}(s, a) $。
2. 计算 $ \epsilon = \max_{s,a} |A^{\pi^k}(s, a)| $ 和 $ C = 4\gamma\epsilon (1-\gamma)^2 $。
3. 随机抽取一个代理的排列 $ i_1:n $。
4. 对于 $ m = 1 : n $，进行以下步骤：
   - 进行更新 $ \pi_{im}^{k+1} = \arg\max_{\pi_{im}} \left[ L_{i1:m}^{\pi^k}(\pi_{i1:m-1}^{k+1}, \pi_{im}) - C D_{\text{max}} \text{KL}(\pi_{im}^k, \pi_{im}) \right] $。

以上是算法1的描述，它描述了一个基于顺序更新的联合策略迭代过程。

我们要强调的是，该算法与简单地对所有代理的联合策略应用TRPO更新方法有明显的区别。首先，我们的算法1不会同时更新整个联合策略，而是按顺序逐个更新每个代理的个体策略。其次，在顺序更新过程中，每个代理都有一个独特的优化目标，该目标考虑了所有先前代理的更新，这也是保持单调改进属性的关键所在。我们通过以下定理证明算法1具有单调改进属性。

定理9：由算法1更新的联合策略序列 $ (\pi^k)_{k=0}^{\infty} $ 具有单调改进属性，即对于所有 $ k \in \mathbb{N} $，有 $ J(\pi^{k+1}) \geq J(\pi^k) $。
证明请参见附录B.2。根据上述定理，我们宣称异质代理信任域学习（HATRL）的成功发展，因为它保留了信任域学习的单调改进属性。此外，我们进一步证明了算法1对于博弈中的渐近收敛性行为。

定理10：假设在算法1中，任何代理的排列都有固定的非零概率开始更新，那么由该算法生成的联合策略序列 $ (\pi^k)_{k=0}^{\infty} $ 在合作马尔可夫博弈中具有非空的极限点集，其中每个极限点都是纳什均衡。证明请参见附录B.3。

在推导这个结果时，算法1引入的新细节起到了重要的作用。单调改进属性（定理9）通过多代理优势函数分解引理和顺序更新方案，为我们提供了对收益收敛的保证。此外，更新顺序的随机化确保在收敛时，没有代理被激励进行更新。通过排除算法在非均衡点收敛的可能性，证明得以完成。

### 3.2 Practical Algorithms

当在实践中实施算法1时，大规模的状态空间和动作空间可能会阻止代理为每个状态$s$单独指定策略$\pi_i(\cdot|s)$。为了解决这个问题，我们通过$\theta_i$对每个代理的策略$\pi_{i\theta_i}$进行参数化，该策略与其他代理的策略一起形成由$\theta = (\theta_1, ..., \theta_n)$参数化的联合策略$\pi_\theta$。在本小节中，我们将开发两种深度多智能体强化学习算法来优化$\theta$。

#### 3.2.1 HATRPO

计算Algorithm 1中的$D_{\text{max KL}}(\pi_{im}^{\theta_{im}^{k}}, \pi_{im}^{\theta_{im}})$是具有挑战性的；它要求在每次迭代中对所有状态评估KL散度。类似于TRPO，可以通过将其替换为预期的KL散度约束$E_{s\sim \rho_{\pi^{\theta_k}}}\left[D_{\text{KL}}(\pi_{im}^{\theta_{im}^{k}}(\cdot|s), \pi_{im}^{\theta_{im}}(\cdot|s))\right] \leq \delta$来缓解最大KL散度惩罚$D_{\text{max KL}}(\pi_{im}^{\theta_{im}^{k}}, \pi_{im}^{\theta_{im}})$，其中$\delta$是一个阈值超参数，而期望值可以通过随机采样轻松近似。通过以上修改，我们提出了实际的HATRPO算法，在每次迭代$k+1$时，给定代理$i_1:n$的一个排列，代理$im\in\{1,...,n\}$按顺序通过最大化受约束目标来优化其策略参数$\theta_{im}^{k+1}$：
$$\theta_{im}^{k+1} = \arg \max_{\theta_{im}} E_{s\sim \rho_{\pi^{\theta_k}}, a_{i1:m-1}\sim \pi_{i1:m-1}^{\theta_{i1:m-1}^{k+1}}, a_{im}\sim \pi_{im}^{\theta_{im}}}\left[A_{im} \pi^{\theta_k}(s, a_{i1:m-1}, a_{im})\right]$$
$$\text{subject to } E_{s\sim \rho_{\pi^{\theta_k}}}\left[D_{\text{KL}}(\pi_{im}^{\theta_{im}^{k}}(\cdot|s), \pi_{im}^{\theta_{im}}(\cdot|s))\right] \leq \delta \quad (7)$$

为了计算上述方程，类似于TRPO，可以对目标函数进行线性近似，对KL约束进行二次近似；通过闭合的更新规则可以解决方程（8）中的优化问题：
$$\theta_{im}^{k+1} = \theta_{im}^k + \alpha_j \sqrt{\frac{2\delta}{g_{im}^k (H_{im}^k)^{-1}g_{im}^k}} (H_{im}^k)^{-1}g_{im}^k$$
其中$H_{im}^k = \nabla_{\theta_{im}^2} E_{s\sim \rho_{\pi^{\theta_k}}} \left[D_{\text{KL}}(\pi_{im}^{\theta_{im}^{k}}(\cdot|s), \pi_{im}^{\theta_{im}}(\cdot|s))\right]\big|_{\theta_{im}=\theta_{im}^k}$是

在HATRPO中，定义$Him_k$为预期KL散度的Hessian矩阵，具体计算公式为：

$$
H_{im_k} = \nabla_{\theta_{im}}^2 E_{s \sim \rho_{\pi_{\theta_k}}} \left[ D_{KL} \left( \pi_{im_{\theta_{im_k}}} (\cdot | s), \pi_{im_{\theta_{im}}} (\cdot | s) \right) \right] \Bigg|_{\theta_{im} = \theta_{im_k}}
$$

其中，$gim_k$为公式(7)中目标函数的梯度，αj < 1是通过回溯线搜索找到的正系数，而$(Him_k)^{-1}gim_k$可以使用共轭梯度算法高效地计算。

估计$E_{a_{i1:m-1} \sim \pi_{i1:m-1}^{\theta_{k+1}}, a_{im} \sim \pi_{im}^{\theta_{im}}} [A_{im} \pi_{\theta_k} (s, a_{i1:m-1}, a_{im})]$是HATRPO中的最后一步，这带来了新的挑战，因为每个智能体的目标函数必须考虑所有先前智能体的更新和输入值的大小。幸运的是，通过引入以下命题，我们可以通过使用联合优势估计器高效地估计这个目标函数。

命题11：假设$π = \prod_{j=1}^{n} \pi_{ij}$是一个联合策略，$A_{\pi}(s, a)$是其联合优势函数。令$\bar{\pi}_{i1:m-1} = \prod_{j=1}^{m-1} \bar{\pi}_{ij}$是智能体i1:m-1的另一个联合策略，$\hat{\pi}_{im}$是智能体im的另一个策略。那么，对于每个状态s，有：

$$
E_{a_{i1:m-1} \sim \bar{\pi}_{i1:m-1}, a_{im} \sim \hat{\pi}_{im}} [A_{im} \pi (s, a_{i1:m-1}, a_{im})] = E_{a \sim \pi} \left[ \left( \frac{\hat{\pi}_{im}(a_{im} | s)}{\pi_{im}(a_{im} | s)} - 1 \right) \frac{\bar{\pi}_{i1:m-1}(a_{i1:m-1} | s)}{\pi_{i1:m-1}(a_{i1:m-1} | s)} A_{\pi}(s, a) \right]
$$

这个命题的好处是，智能体只需要维护一个联合优势估计器$A_{\pi}(s, a)$，而不是为每个单独的智能体维护一个集中的评论家（例如，不像MADDPG等CTDE方法）。另一个实际的好处是，给定优势函数$A_{\pi_{\theta_k}}(s, a)$的估计器$\hat{A}(s, a)$，例如GAE（Schulman等人，2015b），可以使用$(π_{im_{\theta}}(a_{im} | s) \pi_{im_{\theta_k}}(a_{im} | s) - 1) M_{i1:m}(s, a)$的估计器来估计，其中$M_{i1:m} = \frac{\bar{\pi}_{i1:m-1}(a_{i1:m-1} | s)}{\pi_{i1:m-1}(a_{i1:m-1} | s)} \hat{A}(s, a)$。

值得注意，公式（10）与HATRPO中的顺序更新方案非常契合。对于智能体im来说，由于先前的智能体i1:m-1已经进行了更新，因此在公式（10）中M_{i1:m}的联合策略比较容易计算。

给定长度为T的轨迹批次B，我们可以如下估计与策略参数相关的梯度（详见附录C.2）：

$$
\hat{g}_{im_k} = \frac{1}{|B|} \sum_{\tau \in B} \sum_{t=0}^{T} M_{i1:m}(s_t, a_t) \nabla_{\theta_{im}} \log \pi_{im_{\theta_{im}}} (a_t | s_t) \Bigg|_{\theta_{im} = \theta_{im_k}}
$$

公式（10）中的项$-1 \cdot M_{i1:m}(s, a)$在$\hat{g}_{im_k}$中没有反映出来，因为它只引入了一个具有零梯度的常数项。

结合预期KL散度的Hessian矩阵$H_{im_k}$，我们可以通过以下方式更新$\theta_{im_{k+1}}$，根据公式（8）进行更新。

#### HAPPO

为了进一步减轻HATRPO中$H_{im}^k$的计算负担，可以采用PPO的思想，仅考虑使用一阶导数。这可以通过使代理$im$选择一个策略参数$\theta_{im}^{k+1}$，该参数最大化
$$[
E_{s\sim\rho_{\pi_{\theta_k}},a\sim\pi_{\theta_k}}\left[\min\left(\frac{\pi_{im}^{\theta_{im}}(a_i|s)}{\pi_{im}^{\theta_{im}^k}(a_i|s)}M_{i1:m}(s, a), \text{clip}\left(\frac{\pi_{im}^{\theta_{im}}(a_i|s)}{\pi_{im}^{\theta_{im}^k}(a_i|s)}, 1 \pm \epsilon\right)M_{i1:m}(s, a)\right)\right].
]$$
来实现。优化过程可以通过随机梯度方法（如Adam）进行。

### 3.3 Heterogeneous-Agent Mirror Learning: A Continuum of Solutions to Cooperative MARL

最近，Mirror Learning（Kuba等，2022），为TRPO和PPO的有效性提供了一个理论解释，并统一了一类策略优化算法。受到他们的工作的启发，我们进一步提出了针对于合作多智能体强化学习（MARL）的理论框架，命名为异构智能体镜像学习（HAML），它增强了HATRPO和HAPPO的理论保证。作为算法设计的经过验证的模板，HAML将单调改进和NE收敛的期望保证大大推广到了一系列算法，并自然地承载了HATRPO和HAPPO作为其实例，进一步解释了它们的稳健性能。首先，我们介绍HAML属性的必要定义：漂移函数。

定义12：对于$i\in N$，异构智能体漂移函数（HADF）$D_i$由一个映射组成，定义为$D_i : \Pi \times \Pi \times P(-i) \times S \rightarrow \{D_i^\pi(\cdot|s, \bar{\pi}_{j1:m}) : P(A_i) \rightarrow \mathbb{R}\}$，其中对于所有的参数，记作$D_i^\pi(\hat{\pi}_i|s, \bar{\pi}_{j1:m})$，$D_i^\pi(\hat{\pi}_i(\cdot_i|s)|s, \bar{\pi}_{j1:m}(\cdot|s))$，满足以下条件：

1. $D_i^\pi(\hat{\pi}_i|s, \bar{\pi}_{j1:m}) \geq D_i^\pi(\pi_i|s, \bar{\pi}_{j1:m}) = 0$（非负性），
2. $D_i^\pi(\hat{\pi}_i|s, \bar{\pi}_{j1:m})$ 在 $\hat{\pi}_i = \pi_i$ 时具有全局导数为零（零梯度）。

如果对于所有的 $s \in S$，当 $D_i^\pi(\hat{\pi}_i|s, \bar{\pi}_{j1:m}) = 0$ 时能推出 $\hat{\pi}_i = \pi_i$，我们称HADF为正性的。如果对于所有的 $s \in S$，对于所有的 $\pi$、$\bar{\pi}_{j1:m}$ 和 $\hat{\pi}_i$，满足 $D_i^\pi(\hat{\pi}_i|s, \bar{\pi}_{j1:m}) = 0$，我们称HADF为平凡的。直观地说，漂移函数 $D_i^\pi(\hat{\pi}_i|s, \bar{\pi}_{j1:m})$ 是在智能体 $j1:m$ 刚刚更新为 $\bar{\pi}_{j1:m}$ 的情况下，$\pi_i$ 和 $\hat{\pi}_i$ 之间距离的概念。需要强调的是，在这种条件下，相同的更新（从 $\pi_i$ 到 $\hat{\pi}_i$）可能具有不同的大小——这将使得HAML智能体能够以协调的方式柔性地约束其学习步骤。在此之前，我们引入了一个能够提供硬约束的概念，这可能是算法设计的一部分，或者是固有的限制。

定义13：对于$i\in N$，如果对于任意的$\pi_i\in \Pi_i$，存在一个闭球$U_i\pi(\pi_i)$，即存在一个逐状态单调非减的度量$\chi:\Pi_i\times\Pi_i\rightarrow \mathbb{R}$，使得对于任意的$\pi_i\in \Pi_i$，存在$\delta_i>0$，满足$\chi(\pi_i,\bar{\pi}_i)\leq \delta_i \Rightarrow \bar{\pi}_i\in U_i\pi(\pi_i)$，那么我们称$U_i:\Pi\times\Pi_i\rightarrow P(\Pi_i)$为一个邻域算子。对于每个联合策略$\pi$，我们将其与其采样分布关联起来——一个连续依赖于$\pi$的正态分布$\beta_\pi\in P(S)$（参考Kuba等人，2022）。在定义了这些概念之后，我们引入了HAML框架的主要定义。

定义14：对于$i\in N$，$j1:m\in P(-i)$，$D_i$是智能体$i$的一个HADF。异构智能体镜像算子（HAMO）将优势函数集成为$[M(\hat{\pi}_i)D_i,\bar{\pi}_{j1:m}A_\pi](s)=\mathbb{E}_{a_{j1:m}\sim\bar{\pi}_{j1:m},a_i\sim\hat{\pi}_i}[A_i^\pi(s,a_{j1:m},a_i)]-D_i^\pi(\hat{\pi}_i|s,\bar{\pi}_{j1:m})$。注意当$\hat{\pi}_i=\pi_i$时，HAMO的值为零。因此，由于HADF是非负的，改进HAMO的策略$\hat{\pi}_i$必须使其变为正值，从而导致智能体$i$的多智能体优势改进。事实上，在某些配置下，智能体的局部改进会导致所有智能体的联合改进，如引理所述（附录D中证明）。

引理15（HAMO就是你需要的）：设$\pi_{\text{old}}$和$\pi_{\text{new}}$是联合策略，$i_1:n\in \text{Sym}(n)$是智能体的排列。假设对于每个状态$s\in S$和每个$m=1,\ldots,n$，有$[M(\pi_{i_m\text{new}})\cdot D_{i_m},\pi_{i_1:m-1\text{new}}\cdot A_{\pi_{\text{old}}}] \geq [M(\pi_{i_m\text{old}})\cdot D_{i_m},\pi_{i_1:m-1\text{new}}\cdot A_{\pi_{\text{old}}}]$。那么，$\pi_{\text{new}}$是优于$\pi_{\text{old}}$的联合策略，即对于每个状态$s$，$V_{\pi_{\text{new}}}(s) \geq V_{\pi_{\text{old}}}(s)$。随后，联合回报的单调改进性质自然得到，即$J(\pi_{\text{new}}) = \mathbb{E}_{s\sim d}[V_{\pi_{\text{new}}}(s)] \geq \mathbb{E}_{s\sim d}[V_{\pi_{\text{old}}}(s)] = J(\pi_{\text{old}})$。然而，引理的条件要求每个智能体解决$n\times |S|$个不等式（12）的实例，这可能是一个难以解决的问题。我们需要设计一个单一的优化目标，其解满足这些不等式。此外，为了在大规模问题中有实际应用，这样的目标应该可以通过采样进行估计。为了解决这些挑战，我们引入以下算法模板2，生成一系列HAML算法。基于引理15和$\pi_i\in U_i\pi(\pi_i), \forall i\in N, \pi_i\in \Pi_i$的事实，我们可以知道任何HAML算法在每次迭代中（弱地）改进联合回报。在实际设置中，比如深度多智能体强化学习（deep MARL），HAML方法的最大化步骤可以通过对HAMO（参见定义12）的样本平均值进行几步梯度上升来实现。我们还强调，如果邻域算子$U_i$可以选择得到产生较小的策略空间子集，那么得到的更新不仅是改进的，而且还是较小的。这是一个理想的性质，对于优化神经网络策略来说是有帮助的，因为它有助于稳定算法。也许你会想为什么在每次迭代中，HAML更新的智能体顺序是随机的；这个条件对于确保收敛到NE是必要的，这个直观上可以理解：这个随机化过程的固定点联合策略确保没有智能体有动机进行更新，也就是达到NE。在定理16中，我们给出了最基本的HAML性质列表，表明从算法模板2导出的任何方法都可以解决合作多智能体强化学习问题。

定理16（异构智能体镜像学习的基本定理）：对于每个智能体$i\in N$，$D_i$是一个HADF，$U_i$是一个邻域算子，采样分布$\beta_\pi$连续依赖于$\pi$。设$\pi_0\in \Pi$，并且由由$D_i$、$U_i$、$\forall i\in N$和$\beta_\pi$所诱导的一个HAML算法得到了联合策略序列$(\pi_k)_{k=0}^\infty$。那么，由该算法诱导的联合策略具有以下一系列性质：

1. 满足单调改进性质，即$J(\pi_{k+1}) \geq J(\pi_k)$。
2. 它们的值函数收敛到纳什值函数，即$\lim_{k\to\infty} V_{\pi_k} = V_{\text{NE}}$。
3. 它们的期望回报收敛到纳什回报，即$\lim_{k\to\infty} J(\pi_k) = J_{\text{NE}}$。
4. 它们的$\omega$-极限集由纳什均衡组成。

详细证明见附录E。

根据上述定理，我们可以得出结论：HAML提供了一个模板，用于生成在理论上合理、稳定、单调改进的算法，使智能体能够学习解决多智能体合作任务。

在本节中，我们展示了HATRPO和HAPPO实际上是HAML的有效实例，这为它们出色的实证性能提供了更直接的理论解释。我们首先以HATRPO为例，其中代理im（排列i1:n从均匀分布中抽取）更新其策略，以便在满足DKL（πim old，  ̄ πim）≤δ的条件下，最大化以下目标函数：

\[
\text{maximize }\mathbb{E}_{s\sim \rho_{\pi_{\text{old}}},a_{i_1:m-1}\sim \pi_{i_1:m-1\text{new}},a_{\text{im}}\sim \bar{\pi}_{\text{im}}}\left[A_{\pi_{\text{old}}}(s,a_{i_1:m-1},a_{\text{im}})\right]
\]

这可以被表示为一个HAML的最优化目标，其中HADF $D_{\text{im}} \equiv 0$，KL散度邻域算子$U_{\text{im}}^{\pi}(\pi_{\text{im}}) = \{\bar{\pi}_{\text{im}} \mid \mathbb{E}_{s\sim \rho_{\pi}}\left[\text{KL}(\pi_{\text{im}}(\cdot\mid s),\bar{\pi}_{\text{im}}(\cdot\mid s))\right] \leq \delta\}$。HATRPO中使用的采样分布为$\beta_{\pi} = \rho_{\pi}$。最后，由于智能体以随机循环方式更新其策略，该算法是HAML的一个实例。因此，它具有单调改进性质并收敛到纳什均衡集。

在HAPPO中，相对于HATRPO，代理im的更新规则发生了变化，目标函数如下：

\[
\mathbb{E}_{s\sim \rho_{\pi_{\text{old}}},a_{i_1:m-1}\sim \pi_{i_1:m-1\text{new}},a_{\text{im}}\sim \pi_{\text{im old}}}\left[\min\left(r(\bar{\pi}_{\text{im}})A_{i_1:m}^{\pi_{\text{old}}}(s,a_{i_1:m}), \text{clip}\left(r(\bar{\pi}_{\text{im}}), 1\pm \epsilon\right)A_{i_1:m}^{\pi_{\text{old}}}(s,a_{i_1:m})\right)\right]
\]

其中$r(\bar{\pi}_i) = \bar{\pi}_i(a_i\mid s)\pi_{\text{old}}(a_i\mid s)$。我们在附录F中证明了这个优化目标等价于：

\[
\mathbb{E}_{s\sim \rho_{\pi_{\text{old}}},a_{i_1:m-1}\sim \pi_{i_1:m-1\text{new}},a_{\text{im}}\sim \bar{\pi}_{\text{im}}}\left[A_{\pi_{\text{old}}}(s,a_{i_1:m-1},a_{\text{im}}) - \mathbb{E}_{a_{i_1:m-1}\sim \pi_{i_1:m-1\text{new}},a_{\text{im}}\sim \pi_{\text{im old}}}\left[\text{ReLU}\left(\left[r(\bar{\pi}_{\text{im}}) - \text{clip}\left(r(\bar{\pi}_{\text{im}}), 1\pm \epsilon\right)\right]A_{\pi_{\text{old}}}(s,a_{i_1:m-1},a_{\text{im}})\right)\right]\right]
\]

其中紫色的项由于ReLU函数的存在显然是非负的。此外，对于足够接近$\pi_{\text{im old}}$的策略$\bar{\pi}_{\text{im}}$，剪辑算子不会激活，从而使得$r(\bar{\pi}_{\text{im}})$保持不变。因此，紫色的项在$\bar{\pi}_{\text{im}}=\pi_{\text{im old}}$处及其周围的区域为零，这也意味着它的Gâteaux导数为零。因此，它评估了代理im的HADF，使得HAPPO成为HAML的有效实例。

最后，我们想强调HATRPO和HAPPO的这些结论加强了第3.1节和第3.2节中的结果。除了它们在HATRL中的起源外，我们现在还展示了它们的优化目标本身直接享有HAML框架赋予的有利理论性质。这两种解释都支撑了它们的实证性能。


汇报准备

传统的优化问题，loss function最小化--梯度等于0的状态

多智能学习考虑的维度;环境中考虑自己的决策，考虑队友对手对你行为的反馈。

合作博弈问题：

数学表征：
相比于马尔可夫决策，无非是 智能体的数目从1个变到了多个，因此他的action 和reward会从一个变到多个。
objective
智能体的reward以及环境的转移，取决于所有智能体的联合策略。
相应的，每个智能体做Q的迭代就不能只做自己Q learning的update--而是要衡量当前所有智能体的策略行为，把Q全部拿来，再做一个策略评估。
对于一个合作的问题，所有智能体目标一致，评估的Q函数是最大值，采取的策略是能够使得Q最大的策略。
既有合作，又有竞争，要计算一些普世的纳什均衡点。--即当所有的智能体都达到最优。

多智能体强化学习困难在纳什均衡本身很难进行计算。



何为TD erro的迭代？

上面的方法--，值函数分解的方法，不能够涵盖所有的合作博弈

两个智能体之间的合作关系，无法被轻易的解耦，一个智能体必须知道另一个智能体当前的action，才能让他做出正确的决策。

策略梯度的方法：

采用策略梯度的方法，有一个地方非常容易被混淆，比方说，当我求Q的时候，我很难判断Q变大是因为-i的随机探索让其变大，还是a的选择让其变大。

如何衡量新的policy 与旧的policy之间performance的差别：
经过一系列的推导，得到的是advantage fucition的期望，但是，你的action是必须从新的policy Π里面进行采样

置信域做出的很好的创新在于对于performance bound 进行了进一步的简化，surrogate loss函数，使得期望服从于旧有的策略分布，这样的好吃使得更新策略，不从新的policy中采样，而从旧的policy中采样。
比较objective，对于新的任何的一个policy，在并不需要完全达到他的情况下，就可以知道他的performance 。


更新的过程保证新旧policy小于一个固定的罚值。
对L进行一阶泰勒展开，对Dkl进行二姐泰勒展开。

其中g 是policy gradient 。H是一个function，可以把他看成是learning rate 的一个构建的方法。

这个Q函数，只关系一部分智能体的动作，其他的动作直接取期望
原始的A函数，是Q-V..现在的1到m的Q函数相当于整体的Q函数，减去你不关心的这个Q函数--就是V。香蕉味单智能体，相当于一个更加广义的A函数。