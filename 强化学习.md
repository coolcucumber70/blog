# 强化学习学习笔记

## 1、基础知识

### 探索与利用-->多臂老虎机



通过不断的探索使得策略接近于最优策略。

## 2、mdp

### 马尔科夫链

![image-20230314190014824](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230314190014824.png)

### 马尔可夫奖励过程

![image-20230314190303907](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230314190303907.png)

#### 贝尔曼函数

![image-20230314190459626](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230314190459626.png)

### 马尔可夫决策过程

![image-20230314191227197](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230314191227197.png)

#### 马尔可夫决策过程中的策略

![image-20230314192009454](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230314192009454.png)

#### 马尔可夫决策过程中的价值函数

## 3、表格型方法

### 免模型预测

#### 蒙特卡洛策略评估算法

##### eg1

假设有一个小型的强化学习问题，智能体可以在一个 $4 \times 4$ 的网格世界中移动，每个位置上有一个随机生成的奖励值。智能体的目标是在不超过 $10$ 步的情况下，尽可能多地获取奖励。智能体只能采取上下左右四个方向中的一个，到达边界时会反弹回来，如果超过 $10$ 步则任务失败。 现在我们需要使用蒙特卡洛策略评估算法来评估某个随机策略的效果。



蒙特卡洛策略评估算法是用于评估一个给定策略的效果的一种强化学习算法，它通过对智能体和环境进行多次交互来估计状态值函数。

对于这个小型的强化学习问题，我们可以按照以下步骤使用蒙特卡洛策略评估算法来评估某个随机策略的效果：

1. 生成若干条轨迹：从任意一个位置出发，采用给定的随机策略进行移动，直到到达终止状态或者达到最大步数。每一条轨迹包含智能体依次经过的状态和相应的奖励。
2. 对每个状态进行价值估计：对于每个状态 $s$，统计从该状态开始的所有轨迹的奖励和，并将其除以该状态的出现次数，得到该状态的平均奖励，作为该状态的估计值 $V(s)$。
3. 返回状态值函数：将所有状态的估计值组成一个状态值函数 $V$，作为对该随机策略的评估结果。

这个过程可以重复多次，每次都生成一批轨迹，用于更新状态值函数。随着轨迹数的增加，状态值函数的估计值也会越来越精确。

需要注意的是，蒙特卡洛策略评估算法只能用于评估策略，而不能用于改进策略。如果需要改进策略，可以使用基于价值函数的策略改进算法，例如贪心策略或 $\epsilon$-贪心策略。

代码写一下

以下是使用 Python 实现的蒙特卡洛策略评估算法，对于一个随机策略在给定的 $4 \times 4$ 网格世界中进行评估：

```python
pythonCopy codeimport numpy as np

# 定义状态转移函数和奖励函数
def transition(state, action):
    # 返回下一个状态和奖励
    next_state = state + action
    if next_state[0] < 0 or next_state[0] > 3 or next_state[1] < 0 or next_state[1] > 3:
        # 边界反弹
        return state - action, -1
    else:
        # 随机奖励
        reward = np.random.normal(loc=0, scale=1)
        return next_state, reward

# 定义随机策略
def random_policy(state):
    # 在四个方向中随机选择一个动作
    return np.random.choice([(0,1), (0,-1), (1,0), (-1,0)])

# 蒙特卡洛策略评估算法
def monte_carlo_policy_evaluation(policy, max_steps, num_episodes):
    # 初始化状态值函数和状态计数器
    V = np.zeros((4, 4))
    N = np.zeros((4, 4))
    # 进行若干次轨迹生成和状态值更新
    for i in range(num_episodes):
        # 生成一条轨迹
        state = np.random.randint(4, size=2)
        trajectory = []
        for t in range(max_steps):
            action = policy(state)
            next_state, reward = transition(state, action)
            trajectory.append((state, reward))
            if np.array_equal(next_state, state):
                # 达到边界
                break
            state = next_state
        # 更新状态值函数
        G = 0
        for t in range(len(trajectory)-1, -1, -1):
            state, reward = trajectory[t]
            G += reward
            if not any(np.array_equal(state, x) for x in trajectory[:t]):
                N[state[0], state[1]] += 1
                alpha = 1/N[state[0], state[1]]
                V[state[0], state[1]] += alpha * (G - V[state[0], state[1]])
    return V

# 测试
V = monte_carlo_policy_evaluation(random_policy, max_steps=10, num_episodes=1000)
print(V)
```

在这个实现中，我们先定义了状态转移函数和奖励函数，分别用于确定智能体采取某个动作后的下一个状态和奖励。然后定义了随机策略，用于在给定状态下随机选择一个动作。最后实现了蒙特卡洛策略评估算法，用于评估随机策略在给定网格世界中的效果。在测试中，我们使用了 $10$ 步和 $1000$ 条轨迹进行评估，输出了状态值函数的结果。

##### eg2

假设有一个小型的强化学习问题，智能体可以在一个简化的扑克牌游戏中进行决策，每个回合智能体会得到一张扑克牌，然后可以选择抽取下一张牌或者停止抽牌。如果停止抽牌，则当前回合的奖励为当前手牌的点数，如果继续抽牌，则下一张牌的点数会被加到当前手牌的点数上，但是如果手牌的点数超过 $21$ 点，则当前回合的奖励为 $-1$，表示当前局面非常不利。 现在我们需要使用蒙特卡洛策略评估算法来评估某个随机策略的效果。

```python
import numpy as np

# 生成扑克牌
deck = list(range(1, 11)) + [10] * 3

# 定义蒙特卡洛策略评估函数
def monte_carlo_policy_evaluation(policy, num_episodes=10000):
    # 初始化状态价值函数和状态动作计数器
    V = np.zeros(22)
    N = np.zeros(22)
    
    # 对每一轮进行蒙特卡洛策略评估
    for _ in range(num_episodes):
        # 初始化轨迹
        trajectory = []
        state = [np.random.choice(deck), np.random.choice(deck)]
        done = False
        
        # 进行一轮游戏
        while not done:
            action = policy(state)
            trajectory.append((state, action))
            if action == 'stick':
                done = True
            else:
                state.append(np.random.choice(deck))
                if sum(state) > 21:
                    done = True
                    reward = -1
        
        # 计算回报并更新状态价值函数
        G = reward
        for state, _ in reversed(trajectory):
            N[state] += 1
            alpha = 1/N[state]
            V[state] += alpha * (G - V[state])
            G = V[state]
    
    # 返回状态价值函数
    return V[1:21]
```

- 第1行：导入NumPy库，用于处理数学运算。
- 第4行：生成扑克牌列表，包括1到10的牌和3张10点的牌。
- 第6行：定义蒙特卡洛策略评估函数，输入为策略函数和模拟轮数，输出为状态价值函数。
- 第8行：初始化状态价值函数和状态动作计数器，状态价值函数的范围为1到21。
- 第11行：循环进行指定轮数的蒙特卡洛策略评估。
- 第13行：初始化轨迹和初始状态，初始状态为两张随机的牌。
- 第14-17行：开始一轮游戏，当动作为“stick”或者总点数超过21时结束。
- 第19行：计算回报并更新状态价值函数。
- 第20-23行：根据更新公式计算alpha值并更新状态价值函数。
- 第25行：返回状态价值函数，只返回1到21的状态价值函数。

#### 一步时序差分

一步时序差分（one-step temporal-difference，简称TD(0)）是一种强化学习中的算法，它可以从经验中直接学习状态值函数，而不需要知道环境的模型。它的基本思想是，每执行一步动作后，根据当前状态和下一个状态的估计值来更新当前状态的估计值。它的公式如下：

V(S_t) ← V(S_t) + α[R_t+1 + γV(S_t+1) - V(S_t)]

其中，V(S_t)是当前状态S_t的估计值，α是学习率，R_t+1是从S_t到S_t+1的奖励，γ是折扣因子，V(S_t+1)是下一个状态S_t+1的估计值。

##### eg1

一步时序差分（One-step Temporal Difference）是强化学习中的一种算法，其思想是通过比较当前状态下的预测值和实际值来更新状态的价值函数，从而逐步优化策略。

具体来说，一步时序差分算法在每个时间步t上，使用当前状态$S_t$和通过当前策略得到的动作$A_t$来获取下一个状态$S_{t+1}$和对应的奖励$R_{t+1}$，并根据贝尔曼方程的更新规则来更新当前状态的价值函数$V(S_t)$或$Q(S_t, A_t)$。具体地，更新公式如下：
$$
V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
$$
或者
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
$$
其中，$\alpha$是学习率，控制每次更新的幅度；$\gamma$是折扣因子，用于控制未来奖励的重要性。

一步时序差分算法的优点是简单、高效，且具有在线学习的能力，可以在不需要完整的状态转移矩阵的情况下学习状态值或动作值函数。然而，它也存在一些缺点，如随机性较强、更新过程中只考虑了单步奖励等。因此，在实际应用中需要结合其他算法来进一步提升性能。

假设我们有一个简单的强化学习问题，智能体可以在一个 $5 \times 5$ 的网格世界中移动，每个位置上有一个随机生成的奖励值。智能体的目标是从起点 $(0, 0)$ 出发，在不超过 $10$ 步的情况下，尽可能多地获取奖励。智能体只能采取上下左右四个方向中的一个，到达边界时会反弹回来，如果超过 $10$ 步则任务失败。

现在我们需要使用一步时序差分算法来学习一个价值函数，用于评估某个随机策略的效果。

首先，我们可以定义网格世界中的状态 $s$ 如下：

$$s = (x, y)$$

其中 $x$ 和 $y$ 分别表示智能体在网格世界中的横纵坐标。

然后我们可以定义随机策略 $\pi$ 如下：在每个状态下，智能体有相等的概率选择上下左右四个方向中的一个作为下一步的动作。

接下来我们可以定义一个 $5 \times 5$ 的矩阵 $V$ 作为价值函数的初始值，将每个元素初始化为 $0$。

然后我们可以使用一步时序差分算法来学习价值函数。具体来说，我们可以在每个时间步 $t$ 采取以下步骤：

1. 选择一个起始状态 $s_t$，比如 $(0, 0)$。
2. 选择一个动作 $a_t$，根据随机策略 $\pi$ 在状态 $s_t$ 下随机选择一个动作。
3. 执行动作 $a_t$，得到下一个状态 $s_{t+1}$ 和奖励 $r_{t+1}$。
4. 计算时序差分误差 $\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$，其中 $\gamma$ 是折扣因子。
5. 更新价值函数：$V(s_t) \leftarrow V(s_t) + \alpha \delta_t$，其中 $\alpha$ 是学习率。

我们可以重复执行以上步骤，直到收敛到某个值或者达到预定的训练轮数。最终得到的价值函数 $V$ 可以用于评估任意策略的效果。

```python
import numpy as np

# 定义网格世界大小
grid_size = 5

# 定义每个位置上的奖励值
rewards = np.random.normal(loc=0, scale=1, size=(grid_size, grid_size))

# 定义起点和最大步数
start_state = (0, 0)
max_steps = 10

# 定义动作集合
actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]

# 定义值函数和状态访问次数
V = np.zeros((grid_size, grid_size))
N = np.zeros((grid_size, grid_size))

# 定义学习率和折扣因子
alpha = 0.1
gamma = 1.0

# 进行若干次尝试
for episode in range(1000):
    # 初始化状态和步数
    state = start_state
    step = 0

    # 进行一次尝试
    while step < max_steps:
        # 选择动作
        action = actions[np.random.randint(len(actions))]

        # 执行动作并观察新状态和奖励
        next_state = (state[0] + action[0], state[1] + action[1])
        if next_state[0] < 0 or next_state[0] >= grid_size:
            next_state = (state[0] - action[0], state[1] - action[1])
        if next_state[1] < 0 or next_state[1] >= grid_size:
            next_state = (state[0] - action[0], state[1] - action[1])
        reward = rewards[next_state[0], next_state[1]]

        # 更新值函数和状态访问次数
        td_error = reward + gamma * V[next_state[0], next_state[1]] - V[state[0], state[1]]
        N[state[0], state[1]] += 1
        V[state[0], state[1]] += alpha * td_error

        # 更新状态和步数
        state = next_state
        step += 1

        # 判断是否到达边界或者超过最大步数
        if state == start_state or step == max_steps:
            break

```

在上述代码中，我们首先定义了网格世界的大小和每个位置上的奖励值。然后我们定义了起点和最大步数，并且定义了动作集合。接着我们初始化了值函数和状态访问次数，并且定义了学习率和折扣因子。最后我们进行了若干次尝试，在每次尝试中进行一步时序差分更新，即选择动作、执行动作并观察新状态和奖励、更新值函数和状态访问次数，直到到达边界或者超过最大步数为止。

### 免模型控制

#### Sarsa：同策略时序差分控制

Sarsa算法是同策略时序差分控制算法，也被称为“状态-行动-回报-状态-行动（state-action-reward-state-action，SARSA）”算法。Sarsa算法可以用于解决强化学习中的控制问题，即在智能体与环境交互的过程中，如何更新策略以获得最大的长期奖励。

Sarsa算法的基本思路是在同一策略下更新状态-行动值函数 $Q(s,a)$，然后根据更新后的 $Q$ 值函数来更新策略。具体地，Sarsa算法在每个时刻 $t$ 选择行动 $A_t$，根据环境的反馈获得奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$，然后根据当前的策略选择下一个行动 $A_{t+1}$，并利用这些信息来更新状态-行动值函数 $Q(s,a)$：

![image-20230315155945912](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230315155945912.png)

其中，$\alpha$ 是学习率，$\gamma$ 是折扣因子。

在Sarsa算法中，更新策略的方式与ε-贪心算法类似，即在每个时刻以概率 $\epsilon$ 选择随机行动，以概率 $1-\epsilon$ 选择当前最优的行动。最终的策略是在 $Q$ 值函数的基础上得到的。

Sarsa算法具有较好的收敛性和稳定性，并且适用于许多实际应用场景，如机器人控制、游戏智能等。

eg1

```python
import numpy as np

# 定义迷宫大小
n_states = 16
n_actions = 4

# 定义 Q 表格，初始值为 0
Q = np.zeros((n_states, n_actions))

# 定义 epsilon-greedy 策略
epsilon = 0.1
def epsilon_greedy(state):
    if np.random.uniform() < epsilon:
        return np.random.randint(n_actions)
    else:
        return np.argmax(Q[state, :])

# 定义动作和状态转移函数
def step(state, action):
    # 根据当前状态和动作计算下一个状态
    x, y = state // 4, state % 4
    if action == 0:
        next_state = max(x-1, 0)*4 + y
    elif action == 1:
        next_state = min(x+1, 3)*4 + y
    elif action == 2:
        next_state = x*4 + max(y-1, 0)
    else:
        next_state = x*4 + min(y+1, 3)
    # 判断是否到达目标状态或者障碍物
    if next_state == 0:
        reward = 1
    elif next_state == 15:
        reward = 10
    elif next_state == 5 or next_state == 7 or next_state == 11:
        reward = -100
        next_state = state
    else:
        reward = 0
    return next_state, reward

# 定义 Sarsa 算法函数
def sarsa(alpha=0.5, gamma=1.0, num_episodes=100):
    # 记录累计回报
    returns = np.zeros(num_episodes)
    # 进行 num_episodes 轮迭代
    for episode in range(num_episodes):
        state = np.random.randint(1, 15)
        action = epsilon_greedy(state)
        # 记录累计奖励
        returns[episode] = 0
        # 在当前状态下，不断采取动作，直到到达目标状态或者障碍物
        while True:
            next_state, reward = step(state, action)
            next_action = epsilon_greedy(next_state)
            # 更新 Q 值
            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])
            # 累计奖励
            returns[episode] += reward
            # 判断是否到达目标状态或者障碍物，如果是，则结束本轮迭代
            if next_state == 0 or next_state == 15 or next_state == 5 or next_state == 7 or next_state == 11:
                break
            # 更新状态和动作
            state = next_state
            action = next_action
    return Q, returns

```

#### Q学习：异策略时序差分控制

对于走迷宫问题，也可以使用 Q 学习：异策略时序差分控制的方法来求解。在 Q 学习中，我们不需要维护一个状态-动作对的价值函数，而是维护一个动作价值函数 $Q(s,a)$，表示在状态 $s$ 下采取动作 $a$ 可以获得的长期回报。Q 学习的更新规则为：

![image-20230315204533399](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230315204533399.png)

其中 $\alpha$ 为学习率，$\gamma$ 为折扣因子，$S_t$ 和 $A_t$ 分别表示在时间步 $t$ 时的状态和动作，$R_{t+1}$ 表示在时间步 $t+1$ 时的奖励。

相对于 Sarsa 算法，Q 学习不需要在更新时考虑下一步采取的动作，而是选择能够获得最大动作价值的动作来更新。这种方法被称为异策略（Off-policy）时序差分控制。

下面是使用 Q 学习算法求解走迷宫问题的 Python 代码：

```python
import numpy as np
import matplotlib.pyplot as plt

# 定义环境
class Maze:
    def __init__(self):
        self.grid = np.array([
            [0, 0, 0, 0],
            [1, 1, 0, 1],
            [0, 0, 0, 0],
            [0, 1, 1, 0],
            [0, 0, 0, 0]
        ])
        self.start = (0, 0)
        self.goal = (4, 3)

    def reset(self):
        self.current_state = self.start
        return self.current_state

    def step(self, action):
        # 移动
        x, y = self.current_state
        if action == 0:  # 向上移动
            x = max(x - 1, 0)
        elif action == 1:  # 向下移动
            x = min(x + 1, 4)
        elif action == 2:  # 向左移动
            y = max(y - 1, 0)
        else:  # 向右移动
            y = min(y + 1, 3)

        # 判断是否到达终点或陷阱
        if self.grid[x, y] == 0:
            reward = 0
        elif self.grid[x, y] == 1:
            reward = -1
        else:
            reward = 1

        # 更新状态并返回
        self.current_state = (x, y)
        return self.current_state, reward


# Q学习算法
def q_learning(env, num_episodes=500, gamma=0.9, alpha=0.5, eps=0.1):
    n_states, n_actions = env.grid.size, 4
    Q = np.zeros((n_states, n_actions))

    # 统计每个episode的回报
    returns = np.zeros(num_episodes)

    for episode in range(num_episodes):
        # 重置状态
        state = env.reset()

        # 采用epsilon-greedy策略选择行为
        while True:
            if np.random.rand() < eps:
                action = np.random.choice(n_actions)
            else:
                action = np.argmax(Q[state])

            # 执行行为并获得回报和下一个状态
            next_state, reward = env.step(action)

            # 采用epsilon-greedy策略选择下一个行为
            if np.random.rand() < eps:
                next_action = np.random.choice(n_actions)
            else:
                next_action = np.argmax(Q[next_state])

            # 更新Q值
            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])

            # 进入下一个状态
            state = next_state

            # 统计回报
            returns[episode] += reward

            # 如果到达终点或陷阱，则结束
            if env.current_state == env.goal or env.grid[env.current_state] == 1:
                break

    return Q, returns


# 运行Q学习算法并绘制回报曲线
env = Maze()
Q, returns = q_learning(env)
plt.plot(returns)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('Q-Learning in Maze')
plt.show()
```

## 4、策略梯度

### 策略梯度算法

![image-20230316000126986](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230316000126986.png)

![image-20230316000149559](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230316000149559.png)

![image-20230316000224970](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230316000224970.png)

### 策略梯度实现技巧

技巧1：添加基线--本质就是奖励的E期望值

## 5、近端策略优化PPO 算法

![image-20230316003530385](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230316003530385.png)

![image-20230316005733079](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230316005733079.png)

#### 近端策略优化

