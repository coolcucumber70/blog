## 摘要

本文介绍了一种名为Multi-Agent Transformer (MAT)的新型架构，该架构将协作多智能体强化学习（MARL）有效地转化为序列建模问题。MAT将代理人的观察序列编码成潜在表示序列，然后传递给解码器。解码器以顺序和自回归方式生成每个代理人的最佳动作。在训练期间，掩蔽注意力块确保代理人只能访问其前面代理人的动作。我们在两个基准环境上对MAT进行了实验，并与其他方法进行了比较。实验结果表明，MAT在这些环境中取得了最先进的性能，并且可以扩展到更大规模和更复杂的问题中

## 2

### 2.2Multi-Agent Advantage Decomposition Theorem多智能体优势分解定理

Multi-Agent Advantage Decomposition Theorem是一种用于协作多智能体强化学习（MARL）的理论框架，旨在将多智能体策略优化问题转化为单智能体序列决策问题。该定理表明，对于任何联合观测o和联合动作a，都可以通过对代理人的排列进行迭代地分解来计算联合策略π(o,a)。具体而言，该定理表明，对于任何代理人i和其它代理人j（i≠j），在给定前面所有代理人的动作ai_1:i-1的情况下，代理人i的最优动作ai可以独立地计算出来。这个定理提供了一个直观的指导原则，即如何选择逐步改进动作。Multi-Agent Advantage Decomposition Theorem最初由Oliehoek等人在2016年提出，并已成为协作MARL领域中广泛使用的工具之一。

**question1**：下面的这个公式如何理解？
$$
Q_{\pi}(\boldsymbol{o},\boldsymbol{a}^{i_{1:m}})\overset{\Delta}{=}\mathbb{E}\bigl[R^{\gamma}|\mathbf{o}_{0}^{i_{1:n}}=\boldsymbol{o},\mathbf{a}_{0}^{i_{1:m}}=\boldsymbol{a}^{i_{1:m}}\bigr],
$$
![image-20230328113218841](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230328113218841.png)
$$
A^{i_{1,m}}_{\pi}(o,\boldsymbol{a}^{j_{1;h}},\boldsymbol{a}^{i_{1;m}})\triangleq Q^{j_{1;h},i_{1;m}}(\boldsymbol{o},\boldsymbol{a}^{j_{1;h}},\boldsymbol{a}^{i_{1;m}})-Q^{j_{1;h}}_{\boldsymbol{\pi}}(o,\boldsymbol{a}^{j_{1;h}})
$$
![image-20230328112900995](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230328112900995.png)
$$
A_{\pi}^{i_{1:n}}\left(o,a^{i_{1:n}}\right)=\sum_{m=1}^{n}A_{\pi}^{i_m}\left(o,a^{i_{1:m-1}},a^{i_m}\right)
$$
![image-20230328113829990](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230328113829990.png)

### 2.3 Existing Methods in MARL

#### 2.3.1 MAPPO（Multi-Agent Proximal Policy Optimization）

MAPPO（Multi-Agent Proximal Policy Optimization）是一种用于多智能体强化学习的算法，是对 PPO（Proximal Policy Optimization）算法的扩展和改进。

MAPPO 算法主要思想是利用 PPO 的近端策略优化（Proximal Policy Optimization）和多智能体中的可共享知识，通过引入一个经验重放缓冲区来提高策略的稳定性，并使用一种适合多智能体的价值函数，来估计每个智能体在多智能体场景中的贡献。

具体来说，MAPPO 算法将智能体的策略网络和价值网络分别构建为共享网络和私有网络，以利用智能体之间的信息交流。在训练过程中，每个智能体将其个人经验存储到共享的经验重放缓冲区中，以便于其它智能体学习和借鉴。然后，每个智能体使用自己的私有网络更新策略和价值函数，并使用共享的经验重放缓冲区进行样本采样，以提高策略的稳定性和泛化能力。

此外，MAPPO 算法还采用一种适合多智能体的价值函数，即独立Q学习（Independent Q-Learning，IQL）算法。IQL 算法将每个智能体视为独立的个体，使用一个独立的 Q 值函数来估计每个智能体在多智能体环境中的贡献。在 MAPPO 算法中，每个智能体的策略更新都会受到其对应的独立 Q 值函数的影响，从而更好地考虑到每个智能体对整体目标的贡献。

总的来说，MAPPO 算法通过引入共享网络和经验重放缓冲区，以及采用独立Q学习算法来估计每个智能体的贡献，解决了多智能体强化学习中的信息共享和策略稳定性问题，具有较高的训练效率和泛化能力。
$$
\sum_{i=1}^{n}\mathbb{E}_{\textbf{o}\times\varphi_{\pi\theta_{k}},\textbf{a}\sim\pi_{\theta_{k}}}\left[\min\left(\frac{\pi_{\theta}(\textbf{a}^i|\textbf{o})}{\pi_{\theta_{k}}(\textbf{a}^i|\textbf{o})}A_{\pi\theta_{k}}(\textbf{o},\textbf{a}),\textbf{c}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\left(\frac{\pi_{\theta}(\textbf{a}^i|\textbf{o})}{\pi_{\theta_{k}}(\textbf{a}^i|\textbf{o})},1\pm\epsilon\right)A_{\pi\theta_{k}}(\textbf{o},\textbf{a})\right)\right]
$$
这个公式是MAPPO算法中的目标函数，用于更新策略参数 $\theta$。具体来说，它是通过对 $n$ 个采样轨迹上的样本进行加权平均得到的。

其中，$\mathbb{E}*{\textbf{o}\times\varphi*{\pi\theta_{k}},\textbf{a}\sim\pi_{\theta_{k}}}$ 表示在当前策略参数 $\theta_k$ 下的状态和动作的联合分布上取期望。$\textbf{o}$ 和 $\textbf{a}$ 分别表示状态和动作，$\pi_{\theta_k}$ 是当前策略，$\varphi_{\pi_{\theta_k}}(\textbf{o})$ 是在状态 $\textbf{o}$ 下的分布函数。

公式中的第一个 $\min$ 函数中的部分是一种重要的比率估计方法，它被称为 Importance Weighted Actor-Learner Architecture (IMPALA) 中的重要性采样比率。它的含义是在当前策略 $\pi_{\theta_k}$ 下采取动作 $\textbf{a}^i$ 的概率与在目标策略 $\pi_{\theta}$ 下采取同样的动作的概率之比，乘上在当前策略下执行动作 $\textbf{a}^i$ 的优势函数 $A_{\pi\theta_k}(\textbf{o},\textbf{a})$。这个部分的作用是让我们更准确地估计目标策略的价值函数，从而更好地引导策略更新。

公式中的第二个 $\min$ 函数中的部分是一个约束项，用于控制每个采样轨迹上的相对权重。其中，$\textbf{c}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\mathrm{}\left(\frac{\pi_{\theta}(\textbf{a}^i|\textbf{o})}{\pi_{\theta_{k}}(\textbf{a}^i|\textbf{o})},1\pm\epsilon\right)$ 是一个上下界的函数，表示相对权重的范围。如果重要性采样比率大于上界 $1+\epsilon$，则采取当前策略的动作；如果比率小于下界 $1-\epsilon$，则采取目标策略的动作；否则，两个动作都会被考虑，并进行加权平均。

最终，目标函数的值越小，说明目标策略的表现越好，策略参数 $\theta$ 更新得越好。



### 2.4 The Transformer Model

2.4 The Transformer Model是一篇关于序列建模的论文中的一个章节，介绍了Transformer模型。Transformer最初是为机器翻译任务设计的，它采用编码器-解码器结构，其中编码器将输入序列映射到潜在表示，然后解码器以自回归方式生成所需输出序列，在推理的每个步骤中，Transformer将所有先前生成的标记作为输入。Transformer中最重要的组件之一是缩放点积注意力机制，它捕捉输入序列之间的相互关系。该模型已被证明在许多自然语言处理任务中具有优异的性能，并且已被广泛应用于其他领域，如计算机视觉和语音识别等。



### The Surprising Connection Between MARL and Sequence Models

第4部分The Surprising Connection Between MARL and Sequence Models讲了协作多智能体强化学习（MARL）和序列模型之间的联系。该部分介绍了Multi-Agent Advantage Decomposition Theorem，它提供了一种新的理解MARL问题的方法，即从序列模型的角度来看待问题。该定理表明，如果每个代理人都知道其前任代理人的动作，并且这些动作具有任意决策顺序，则代理人们的局部优势之和将恰好等于联合优势。这种有序决策设置简化了联合策略更新过程，其中最大化每个代理人自己的局部优势等价于最大化联合优势。基于此定理，该论文提出了一种新的MARL训练范式，并引入了Multi-Agent Transformer（MAT），这是一种实现通用MARL解决方案的编码器-解码器架构

# 项目内容

main函数

```python
def main():
    params = deepcopy(sys.argv)  # 复制sys.argv到params
    # 从default.yaml获取默认值
    with open(os.path.join("../data", "config", "default.yaml"), "r", encoding="utf-8") as f:
        try:
            config_dict = yaml.load(f)  # 加载yaml文件
        except yaml.YAMLError as exc:
            assert False, "default.yaml error: {}".format(exc)  # 如果加载失败，抛出异常
    # 获取分布式配置和算法配置的名称
    dist_name = _get_config_name(params, "--dist-config")
    alg_name = _get_config_name(params, "--alg-config")
    # 如果没有指定分布式配置，则使用默认值中的分布式类型
    if not dist_name:
        dist_name = config_dict['dist_type']
    # 如果没有指定算法配置，则使用默认值中的红方智能体
    if not alg_name:
        alg_name = config_dict['red_agent']
    # 获取分布式和算法的配置
    dist_config = _get_config(dist_name, "dist")
    alg_config = _get_config(alg_name, "algs")
    # 递归地更新配置字典
    config_dict = recursive_dict_update(config_dict, dist_config)
    config_dict = recursive_dict_update(config_dict, alg_config)
    # 将配置字典转换为SN命名元组
    arglist = SN(**config_dict)
    # 解析配置文件
    parser = Parser(1, 1, arglist)
    parser.parser_json(arglist.load_scheme_path)
    ####---- blue agent
    # 判断是否给定蓝方规则树
    for i in arglist.blue_agent:
        if i in ['zc', 'jkbd']:
            arglist.tree_agent_flag = 1
            break
    # 初始化蓝方
    arglist.blue_agent_class = []
    if arglist.tree_agent_flag:  # 目前联盟博弈不支持规则树，TODO 支持规则树
        for i in range(len(arglist.blue_agent)):
            arglist.blue_agent_class.append(importlib.import_module('blue.{}.blue_agent'.format(arglist.blue_agent[i])).Agent)
    else:  # 联盟博弈或单独训练蓝方，初始化蓝方
        arglist.blue_agent_class = importlib.import_module('blue.{}.agents.blue_agent'.format(arglist.blue_agent[0])).Agent
        arglist.sampler_class_blue = importlib.import_module('blue.{}.algorithms.sampler'.format(arglist.blue_agent[0])).Sampler
        arglist.learner_class_blue = importlib.import_module('blue.{}.algorithms.learner'.format(arglist.blue_agent[0])).Learner
        arglist.buffer_class_blue = importlib.import_module('blue.{}.algorithms.buffer'.format(arglist.blue_agent[0])).Buffer
    ####----
    # 初始化红方
    arglist.red_agent_class = importlib.import_module('red.agents.{}.red_agent'.format(arglist.red_agent)).Agent
    arglist.sampler_class = importlib.import_module("red.algorithms.{}.sampler".format(arglist.red_agent)).Sampler
    arglist.learner_class = importlib.import_module("red.algorithms.{}.learner".format(arglist.red_agent)).Learner
    arglist.buffer_class = importlib.import_module("red.algorithms.{}.buffer".format(arglist.red_agent)).Buffer
    # 根据游戏类型选择环境类
    if arglist.env_type == 'sc2':
        from red.algorithms import REGISTRY as envGISTRY
        arglist.env_class = envGISTRY['sc2']
    else:
        arglist.env_class = ScenarioEnv
    # 选择训练方式并执行训练
    print("start to train")
    if arglist.play_form in ['red_only']:
        if arglist.sample_type in ['normal']:
            from train_normal import maintrain
        else:
            import spirit
            from spirit.job_config import JobConfig
            if arglist.mpts_sitation in ['k8s']:
                spirit.util.connect("10.18.10.10:10001", job_config=JobConfig(runtime_env={'working_dir': "."}))
            elif arglist.mpts_sitation in ['docker','pc']:
                spirit.init()
            print("connect to spirit succeed!")
            from spirit.sllib.core.train_dist import maintrain
        if arglist.use_cuda:
            arglist.device = th.device("cuda:0" if th.cuda.is_available() else "cpu")
            th.cuda.set_device(arglist.device)
        else:
            arglist.device = th.device("cpu")
    elif arglist.play_form in ['blue_only']:
        from train_normal_blue import maintrain
    elif arglist.play_form in ['pe']:
        from train_normal_pe import maintrain
    maintrain(arglist)
    
```

这段代码的功能是根据不同的参数值调用不同的Python模块中的maintrain函数进行训练。具体的解释如下：

if arglist.play_form in ['red_only']:：当参数play_form的取值为red_only时，执行以下代码块。

if arglist.sample_type in ['normal']:：当参数sample_type的取值为normal时，从train_normal模块中导入maintrain函数。

else:：当参数sample_type的取值不为normal时，执行以下代码块。

import spirit：导入spirit模块。

from spirit.job_config import JobConfig：从spirit.job_config模块中导入JobConfig类。

if arglist.mpts_sitation in ['k8s']:：当参数mpts_sitation的取值为k8s时，调用spirit.util.connect函数连接到指定的IP地址和端口号，并使用JobConfig类对象指定运行环境的工作目录为当前目录。

elif arglist.mpts_sitation in ['docker','pc']:：当参数mpts_sitation的取值为docker或pc时，调用spirit.init函数初始化Spirit。

print("connect to spirit succeed!")：输出一条连接Spirit成功的信息。

from spirit.sllib.core.train_dist import maintrain：从spirit.sllib.core.train_dist模块中导入maintrain函数。

if arglist.use_cuda:：当参数use_cuda的值为True时，执行以下代码块。

arglist.device = th.device("cuda:0" if th.cuda.is_available() else "cpu")：根据CUDA是否可用，将arglist.device设置为cuda:0或cpu。

th.cuda.set_device(arglist.device)：将当前设备设置为arglist.device指定的设备。

else:：当参数use_cuda的值为False时，执行以下代码块。

arglist.device = th.device("cpu")：将arglist.device设置为CPU设备。

elif arglist.play_form in ['blue_only']:：当参数play_form的取值为blue_only时，从train_normal_blue模块中导入maintrain函数。

elif arglist.play_form in ['pe']:：当参数play_form的取值为pe时，从train_normal_pe模块中导入maintrain函数。

maintrain(arglist)：调用导入的maintrain函数，参数为arglist

```python
def maintrain(arglist):
    RedAgentClass = arglist.red_agent_class  # 红方智能体类
    BlueAgentClass = arglist.blue_agent_class  # 蓝方智能体类
    learner_red_agent = RedAgentClass(arglist)  # 创建红方智能体对象
    learner_blue_agent = BlueAgentClass(arglist)  # 创建蓝方智能体对象
    if arglist.env_type == 'sc2':  # 如果环境类型为StarCraft2
        learner_env = arglist.env_class(**arglist.env_args)  # 创建StarCraft2环境对象
        learner_red_agent.setup(learner_env.get_env_info(), None)  # 配置红方智能体
    else:
        ScenarioEnv = arglist.env_class  # 创建自定义环境类
        learner_env = ScenarioEnv(arglist, sampler_id=0)  # 创建自定义环境对象
        learner_red_agent.setup(learner_env.entities_red,  # 配置红方智能体
                                group_actions=learner_env.group_actions)
        learner_blue_agent.setup(learner_env.entities_blue,  # 配置蓝方智能体
                                 group_actions=learner_env.group_actions)
        if arglist.load_model_path:  # 如果指定了加载红方智能体模型的路径
            print("Loading Red agent model:  %s" % arglist.load_model_path)  # 输出加载红方智能体模型的信息
            learner_red_agent.load_model(arglist.load_model_path)  # 加载红方智能体模型
        if arglist.load_blue_model_path:  # 如果指定了加载蓝方智能体模型的路径
            print("Loading Blue agent model: %s" %
                  arglist.load_blue_model_path)  # 输出加载蓝方智能体模型的信息
            learner_blue_agent.load_model(arglist.load_blue_model_path)  # 加载蓝方智能体模型
        if arglist.mpts_sitation in ['k8s', 'docker']:  # 如果运行环境为k8s或docker
            learner_env.base.get_host_handel.close()  # 关闭连接
    print('learner red agent ready...')  # 输出红方智能体准备就绪的信息
    print('learner blue agent ready...')  # 输出蓝方智能体准备就绪的信息
    buffer = arglist.buffer_class_blue(arglist)  # 创建回放缓存对象
    print("Replay Buffer ready...")  # 输出回放缓存准备就绪的信息
    sampler = arglist.sampler_class_blue(sampler_id=0, red_agent_class=RedAgentClass, blue_agent_class=BlueAgentClass, input_args=arglist)  # 创建采样器对象
    print("Sampler Ready...")  # 输出采样器准备就绪的信息
    learner = arglist.learner_class_blue(args=learner_blue_agent.args,
                           net=learner_blue_agent.net)  # 创建蓝方学习器对象
    print("Learner Ready...")  # 输出蓝方学习器准备就绪的信息
    LoggerClass = Logger  # 创建日志记录器类对象
```

