# Beyond Reward: Offline Preference-guided Policy Optimization

摘要：

OPPO achieves this by introducing an offline hindsight information matching objective for optimizing a contextual policy and a preference modeling objective for finding the optimal context. 

离线偏好引导策略优化（OPPO）范式

这项研究关注离线偏好强化学习（PbRL）的主题，它是传统强化学习的一种变体，不需要在线交互或明确指定奖励函数。相反，代理器被提供了预先存在的离线轨迹和人类对轨迹对之间的偏好，以提取动态和任务信息。由于动态和任务信息是正交的，一个朴素的方法将涉及使用基于偏好的奖励学习，然后使用现成的离线强化学习算法。然而，这需要单独学习一个标量奖励函数，这被认为是信息瓶颈。为了解决这个问题，我们提出了离线偏好引导策略优化（OPPO）范式，它在一个步骤中对离线轨迹和偏好进行建模，消除了单独学习奖励函数的需要。OPPO通过引入离线顺见信息匹配目标来实现这一点，用于优化上下文策略，并引入偏好建模目标来找到最佳的上下文。OPPO通过迭代优化这两个目标进一步整合了一个表现良好的决策策略。我们的实证结果表明，OPPO能够有效地模拟离线偏好，并胜过先前的竞争基准，包括基于真实或伪造奖励函数规范进行的离线强化学习算法。

introduction

在离线偏好强化学习中，学习器可以获取离线数据集以及离线轨迹对之间的标记偏好。常见的方法是将在线偏好强化学习方法与现成的离线强化学习算法相结合。然而，单独学习一个解释专家偏好的奖励函数可能不能直接指导策略如何最优地行动。这是因为偏好标签定义了PbRL任务，目标是学习注释者最喜欢的轨迹，而不是最大化策略展开的累积折扣代理奖励。在复杂任务（如非马尔科夫任务）中，标量奖励可能会导致策略改进的信息瓶颈，导致次优行为。此外，孤立的策略优化可能会利用错误校准的奖励函数中的漏洞，导致不良行为。因此，有理由质疑学习奖励函数的必要性，尤其是考虑到它可能不能直接产生最优策略。

![image-20230527110826816](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230527110826816.png)

为了实现这一目标，研究提出了离线偏好引导策略优化（OPPO）方法，它是一个一步法的范式，可以同时建模离线偏好并学习最优决策策略，而不需要单独学习奖励函数。通过使用两个目标：离线顺见信息匹配目标和偏好建模目标，通过迭代优化这些目标，得到一个上下文策略 π(a|s, z) 来模拟离线数据，并得到一个最优上下文 z∗ 来模拟偏好。OPPO的主要重点是学习一个高维的 z 空间，并在该空间内评估策略。这个高维的 z 空间捕捉了比标量奖励更多的与任务相关的信息，使其成为一种更有效的方法。最终，通过将上下文策略 π(a|s, z∗) 条件化得到最优策略。

该研究的主要贡献可以总结如下：首先，提出了OPPO，一个简洁、稳定且一步法的离线PbRL范式，避免了单独学习奖励函数的需要。其次，提出了一种基于偏好的顺见信息匹配目标和关于上下文的新颖偏好建模目标。最后，进行了大量实验证明了OPPO相对于以前的竞争基准的优越性，并对其性能进行了分析

OPPO: Offline Preference-guided Policy Optimization



3. ## Preliminaries

$$
P[\tau^i\succ\tau^j]=\mathrm{logistic}(\sum_t r_\psi(\mathbf{s}_t^i,\mathbf{a}_t^i)-\sum_t r_\psi(\mathbf{s}_t^j,\mathbf{a}_t^j))
$$

这个式子表示轨迹$\tau^i$优于轨迹$\tau^j$的概率，其中$\mathrm{logistic}(x)$表示逻辑函数，$\mathbf{s}_t^i$和$\mathbf{a}*t^i$分别表示轨迹$\tau^i$在时刻$t$的状态和动作，$r*\psi(\mathbf{s}_t^i,\mathbf{a}_t^i)$表示在状态$\mathbf{s}_t^i$采取动作$\mathbf{a}*t^i$的偏好一致的假设奖励，$\sum_t r*\psi(\mathbf{s}_t^i,\mathbf{a}*t^i)$表示轨迹$\tau^i$的假设奖励总和，$\sum_t r*\psi(\mathbf{s}_t^j,\mathbf{a}*t^j)$表示轨迹$\tau^j$的假设奖励总和。这个式子的含义是，在假设的奖励函数$r*\psi$下，轨迹$\tau^i$相对于轨迹$\tau^j$更有可能导致优于的偏好的，以下是对原文的直接翻译：



其中$(s_{i,t}, a_{i,t})\sim \tau_i, (s_{j,t}, a_{j,t})\sim \tau_j$。从直觉上来看，这个式子可以被解释为：优选一个轨迹的概率指数地依赖于标记的轨迹上的累积奖励。因此，我们可以将假设的奖励函数视为一个标签函数，它可以将状态和动作映射到一个实数值，该值表示在该状态下采取该动作的价值。我们的目标是通过最小化以下交叉熵损失函数来更新假设奖励函数：

使用学习到的奖励函数$r_\psi$标记数据集中的每个转移后，我们可以采用现成的离线强化学习算法来实现策略学习。

(一旦学习到奖励函数$r_\psi$并将其用于标记数据集中的每个转移，我们就可以采用现成的离线强化学习算法来实现策略学习。这些算法可以使用标准的强化学习框架，但是将奖励替换为假设的奖励函数$r_\psi$。这使得我们能够在没有实时交互的情况下学习策略。常见的离线强化学习算法包括离线Q-learning和离线策略评估。这些算法使用假设的奖励函数来计算每个状态-动作对的值函数，然后使用值函数来更新策略。经过训练，我们可以获得一个最优策略$\pi_\psi$，它在假设的奖励函数下实现最优性，即$\pi_\psi=\arg\max_\pi J_r(\pi)=\arg\max_\pi J_{r_\psi}(\pi)$，其中$J_r(\pi)$和$J_{r_\psi}(\pi)$分别表示在真实奖励函数$r$和假设奖励函数$r_\psi$下的期望回报。需要注意的是，由于假设奖励函数$r_\psi$是在离线数据集上学习的，它可能无法完全捕捉真实环境中的奖励信号，因此得到的策略可能在实际环境中表现不佳。因此，我们需要在实际环境中进行一些形式的在线调整，例如在实时交互中使用一些探索机制，以便探索未知的状态-动作对，从而改进策略的性能。)

3.2. 后见信息匹配
除了典型的迭代（离线）RL框架外，信息匹配（IM）（Furuta等人，2021）最近被研究作为（离线）RL的替代问题规范。RL中IM的目标是学习一个上下文策略$\pi(a|s, z)$，其轨迹回溯满足预定义的期望信息统计值$z$：
$$
\min\limits_{\pi\underset{\tau_\mathbf{z}\sim p(\mathbf{z})}{\mathbb E}}\left[\ell\left(\mathbf{z},I(\tau_\mathbf{z})\right)\right],(3)
$$
在此式中，$p(z)$是一个先验分布，$\pi(z)$表示在环境中回溯$\pi(a|s, z)$生成的轨迹分布。$I(\tau)$是一个函数，用于捕捉轨迹$\tau$的统计信息，例如状态和奖励的分布统计信息，如均值、方差（Wainwright等人，2008），$\ell$是一个损失函数。

一方面，如果我们将$p(z)$设为先验分布，则优化公式(3)相当于执行无监督（在线）RL，学习一组技能（Eysenbach等人，2018; Sharma等人，2019）。另一方面，如果我们将$p(z)$设为给定的离线策略轨迹（或状态-动作）分布$D(\tau)$（或$D(s,a)$）的统计信息，则公式(3)对于（离线）RL中的后见信息匹配表示一个目标。例如，HER（Andrychowicz等人，2017）和回报条件RL（倒置RL）（Srivas-
tava等人，2019; Kumar等人，2019b; Chen等人，2021; Janner等人，2021）使用了上述后见信息的概念：将数据集中的任何轨迹$\tau$指定为后见目标，并将公式(3)中的信息$z$设置为$I(\tau)$。然后，可以使用公式(3)的目标来训练上下文策略，以实现在给定的后见目标下的最优性。我们提供了I(·)-驱动的后见信息匹配（HIM）目标函数。

![image-20230527143505538](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230527143505538.png)(4)

其中$z:=I(\tau)$。在HER中，我们将$I(\tau)$设置为轨迹$\tau$的最终状态，在回报条件RL中，我们将$I(\tau)$设置为轨迹$\tau$的回报。因此，我们可以使用后见信息$z:=I(\tau)$来提供监督来训练上下文策略$\pi(a|s,z)$。然而，在离线设置中，从$\pi(z)$中采样$\tau_z$是不可访问的。因此，除了I(·)-驱动的后见信息建模之外，我们必须对环境转移动力学进行建模。也就是说，我们需要对轨迹本身进行建模，即$\min_\pi \mathbb{E}_{\tau\sim D(\tau),\tau_z\sim \pi(z)}[\ell(\tau,\tau_z)]$。因此，我们提供完整的离线HIM目标：

![image-20230527143737560](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230527143737560.png)(5)

为了直观理解上述目标函数，我们提供一个简单的例子：考虑后见信息$I(\cdot)$为轨迹的回报，在优化$\ell(I(\tau),I(\tau_z))$的过程中，确保生成的$\tau_z$的回报与$\tau=I^{-1}(z)$相同。然而，在离线设置中，我们必须确保生成的$\tau_z$保持在离线数据的支持范围内，消除了分布外（OOD）问题。因此，我们近似地最小化$\ell(\tau,\tau_z)$。在实现中，直接优化$\ell(\tau,\tau_z)$就足以确保后见信息匹配，例如，$\ell(I(\tau),I(\tau_z))<\epsilon$。在这里，我们明确地形式化了$\ell(I(\tau),I(\tau_z))$项，特别强调了后见信息匹配目标的必要性，同时强调了上述HIM目标（将$I(\cdot)$视为先验）与我们的OPPO公式（要求学习$I_\theta(\cdot)$）之间的区别，详见第4节。通过优化公式(5)，我们可以获得一个上下文策略$\pi(a|s,z)$。在评估阶段，可以通过对所选目标$z^*$对策略进行条件化来指定最优策略$\pi(a|s,z^*)$。例如，Decision Transformer（Chen等人，2021）将期望性能作为目标$z^*$（例如，指定最大可能回报以生成专家行为），而RvS-G（Emmons等人，2021）将目标状态作为目标$z^*$。

4

在本节中，我们介绍了我们的方法OPPO（offline preference-guided policy optimization），它采用了第4.1节中的后见信息匹配（HIM）目标来建模离线上下文策略$\pi(a|s, z)$，并在第4.2节引入了三元组损失来建模人类偏好以及最优上下文$z^*$。在测试阶段，我们将策略条件化为最优上下文$z^*$，并使用$\pi(a|s, z^*)$进行轨迹回溯。原则上，OPPO适用于任何PbRL设置，包括在线和离线。然而，在我们的分析和实验范围内，我们专注于离线设置，以分离在线RL中的探索难题。

4.1. 基于HIM的策略优化
如第3.1节所述，为了直接实现现成的离线RL算法，以前的PbRL工作明确学习一个奖励函数，使用公式(2)（如图1左侧所示）。作为这种两步方法的替代方案，我们试图直接从偏好标记的离线数据集中学习策略（如图1右侧所示）。受第3.2节离线HIM目标的启发，我们提出在离线PbRL设置中学习上下文策略$\pi(a|s, z)$。假设$I_\theta$是一个（可学习的）网络，它对PbRL中的后见信息进行编码，我们制定以下目标：
$$
\min\limits_{\pi,I_{\theta}}\mathcal{L}*{\mathrm{HM}}:=\underset{\tau\sim\mathcal{D}(\tau)}{\mathbb{E}}\left[\ell\left(I*{\theta}(\tau),I_{\theta}(\tau_{\mathbf{z}})\right)+\ell\left(\tau,\tau_{\mathbf{z}}\right)\right]
$$
![image-20230527144530298](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230527144530298.png)

(在此式中，$\mathcal{L}*{HM}$表示HIM损失函数，$\tau$表示从数据集$\mathcal{D}(\tau)$中采样得到的轨迹，$\ell(\cdot,\cdot)$是距离度量函数，用于比较轨迹和后见信息之间的相似性。具体来说，第一项$\ell\left(I*{\theta}(\tau),I_{\theta}(\tau_{\mathbf{z}})\right)$表示在后见信息编码空间中测量轨迹$\tau$和最优上下文$z^*$对应的轨迹$\tau_{\mathbf{z}}$之间的距离；第二项$\ell\left(\tau,\tau_{\mathbf{z}}\right)$表示在状态-动作空间中测量轨迹$\tau$和$\tau_{\mathbf{z}}$之间的距离。通过最小化$\mathcal{L}*{HM}$，我们可以同时学习上下文策略$\pi(a|s, z)$和后见信息网络$I*\theta$。)

在这里，$z:=I_\theta(\tau)$。需要注意的是，公式(6)是公式(5)的另一种实例化形式，在其中我们在PRBL环境中学习后见信息提取器$I_\theta(\cdot)$，而以前的（离线）RL算法通常将$I(\cdot)$设为先验（Chen等人，2021; Emmons等人，2021）。这样的编码器-解码器结构现在类似于Furuta等人（2021）为离线模仿学习提出的双向决策Transformer（BDT）。然而，在PbRL环境中缺少专家演示，因此在第4.2节中，我们建议使用D≻中的**偏好标签**来提取后见信息。

