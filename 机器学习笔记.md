# 机器学习笔记

## 第一部分：分类

### 1、knn算法

```python
def classify0(inX, dataSet, labels, k):
    # 获取数据集大小，即数据集中样本的数量
    dataSetSize = dataSet.shape[0]
    # 计算输入样本和数据集中每个样本之间的距离
    # 每个距离都是一个列向量，diffMat是一个矩阵，共有dataSetSize行，每行是输入样本和一个数据集样本之间的差
    diffMat = tile(inX, (dataSetSize,1)) - dataSet
    # 计算每个差的平方
    sqDiffMat = diffMat**2
    # 将每行上的平方和求和，得到每个样本与输入样本的距离的平方
    sqDistances = sqDiffMat.sum(axis=1)
    # 对每个平方距离取平方根，得到每个样本与输入样本的距离
    distances = sqDistances**0.5
    # 按照距离从小到大对样本进行排序，返回排序后的样本下标
    sortedDistIndicies = distances.argsort()     
    # 选择距离最近的k个样本
    classCount={}          
    for i in range(k):
        # 获取样本的类别
        voteIlabel = labels[sortedDistIndicies[i]]
        # 将类别存入字典中，统计出现次数
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
    # 对字典中的类别出现次数进行排序，得到出现次数最多的类别
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    # 返回出现次数最多的类别
    return sortedClassCount[0][0]
该代码中主要的步骤如下：

计算输入样本和数据集中每个样本之间的欧氏距离，得到一个距离向量；
将距离向量从小到大排序，取其中距离最小的k个样本；
统计这k个样本中每个类别出现的次数，并选择出现次数最多的类别作为输入样本的类别。
该KNN分类器的核心思想是：与输入样本距离最近的k个样本中，出现次数最多的类别很有可能是输入样本所属的类别。
```

其中：

**classCount.get(voteIlabel,0)上述的get是怎么用的？**

`get()` 是字典类型的方法，用于获取指定键的值。它的语法是 `d.get(key, default=None)`，其中 `d` 是字典类型的变量，`key` 是字典中的键，`default` 是默认值，如果指定的键不存在，则返回默认值。如果未指定默认值，则返回 `None`。

在上述代码中，`classCount` 是一个字典类型的变量，键是样本的类别，值是该类别在k个最近邻样本中出现的次数。`voteIlabel` 是一个样本的类别，`classCount.get(voteIlabel, 0)` 表示从字典 `classCount` 中获取键 `voteIlabel` 的值，如果该键不存在，则返回默认值 0。

如果 `voteIlabel` 对应的类别已经出现在字典 `classCount` 中，`classCount.get(voteIlabel,0)` 返回该类别在字典中对应的值，然后将该值加1。如果 `voteIlabel` 对应的类别还没有出现在字典 `classCount` 中，则返回默认值 0，然后将该值加1，将该类别作为新的键存入字典 `classCount` 中，并将值初始化为1。因此，`classCount.get(voteIlabel, 0) + 1` 表示将样本类别 `voteIlabel` 对应的值加1，即更新样本类别在k个最近邻样本中出现的次数。



**sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)sorted函数怎么用的？**



`sorted()` 是一个内置函数，用于对可迭代对象进行排序。它有三个参数：

- `iterable`：待排序的可迭代对象。
- `key`：用于排序的函数，它接受一个元素作为参数并返回一个用于比较的值。如果未指定，则默认使用元素本身进行比较。
- `reverse`：指定是否降序排列。默认为 `False`，表示升序排列。

在上述代码中，`classCount` 是一个字典类型的变量，它的键是类别，值是该类别在k个最近邻样本中出现的次数。为了进行排序，需要将字典类型转换为列表类型，每个元素是一个元组，其中第一个元素是键，第二个元素是值。这里使用 `classCount.iteritems()` 将字典类型转换为迭代器，迭代器中的每个元素是一个元组 `(key, value)`，然后将其转换为列表类型。

使用 `key=operator.itemgetter(1)` 指定按照元组的第二个元素（即值）进行比较，`reverse=True` 指定按照降序排列。因此，`sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)` 表示对字典 `classCount` 中的键值对进行排序，按照值从大到小排列，返回一个列表类型的排序结果。排序结果中每个元素是一个元组，其中第一个元素是类别，第二个元素是该类别在k个最近邻样本中出现的次数。

### 2、决策树算法

```python
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    labelCounts = {}
    for featVec in dataSet: #the the number of unique elements and their occurance
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob,2) #log base 2
    return shannonEnt
    
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet
    
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain = 0.0; bestFeature = -1
    for i in range(numFeatures):        #iterate over all the features
        featList = [example[i] for example in dataSet]#create a list of all the examples of this feature
        uniqueVals = set(featList)       #get a set of unique values
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = len(subDataSet)/float(len(dataSet))
            newEntropy += prob * calcShannonEnt(subDataSet)     
        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy
        if (infoGain > bestInfoGain):       #compare this to the best gain so far
            bestInfoGain = infoGain         #if better than current best, set to best
            bestFeature = i
    return bestFeature                      #returns an integer

def majorityCnt(classList):
    classCount={}
    for vote in classList:
        if vote not in classCount.keys(): classCount[vote] = 0
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]

def createTree(dataSet,labels):
    classList = [example[-1] for example in dataSet]
    if classList.count(classList[0]) == len(classList): 
        return classList[0]#stop splitting when all of the classes are equal
    if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)
    bestFeatLabel = labels[bestFeat]
    myTree = {bestFeatLabel:{}}
    del(labels[bestFeat])
    featValues = [example[bestFeat] for example in dataSet]
    uniqueVals = set(featValues)
    for value in uniqueVals:
        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)
    return myTree                            
    
```

**列表推导式语法怎么用？**

列表推导式是一种用来创建新的列表的方法，它通常比使用循环语句更为简洁和高效。其基本语法格式如下：

```python
csharpCopy code
[expression for item in iterable if condition]
```

其中，`expression` 是用来计算每个元素的表达式，`item` 是要迭代的每个元素，`iterable` 是一个可迭代的对象，如列表、元组、集合等，`condition` 是一个可选的条件语句，用于过滤元素。该语句将返回一个新的列表，其中包含对每个元素应用 `expression` 后的结果。

例如，如果我们想要创建一个列表，其中包含 1 到 10 中的所有偶数的平方，可以使用以下列表推导式：

```python
pythonCopy code
squares = [x**2 for x in range(1, 11) if x % 2 == 0]
```

该语句将返回一个新的列表 `[4, 16, 36, 64, 100]`，其中包含 1 到 10 中的所有偶数的平方。

**myTree = {bestFeatLabel:{}}什么语法**

这是Python中的字典（dictionary）类型的定义语法。该语法用花括号 {} 表示，每个键值对之间用冒号 : 分隔，每个键值对之间用逗号 , 分隔。其中，键表示一个字符串类型的标签，值可以是任何数据类型，包括字典类型、列表类型、元组类型等。这里的语法表示定义一个空的字典，它的唯一一个键值对的键为bestFeatLabel，值为空的字典类型。

这行代码是从标签列表中删除已经选取的最佳特征，以确保在递归构建决策树时不会重复使用相同的特征。在递归调用createTree函数时，会将标签列表传递到下一层，因此需要将已经使用的特征从标签列表中删除。

**uniqueVals = set(featValues)什么语法**

这是Python中的set数据类型，它是一种无序且不重复的集合，可以用于去除列表中的重复元素。在这行代码中，set()函数将featValues转换为一个集合，uniqueVals变量将包含featValues中所有不同的元素。

  **myTree[bestFeatLabel] [value] 这是什么语法？**

这是字典嵌套的语法。`myTree` 是一个字典类型的数据结构，`bestFeatLabel` 是该字典的键（key），值（value）是另一个字典，该字典的键是 `value`，即来自数据集中某一特征的唯一取值，而值则是由递归调用 `createTree()` 函数生成的子树（subtree）。这种嵌套的方式可以形成一棵树形结构。--他这里竟然用字典来表示树，真的是让人想不到可以这样啊。

使用matlibplot画图

```python
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            numLeafs += getNumLeafs(secondDict[key])
        else:   numLeafs +=1
    return numLeafs

def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:   thisDepth = 1
        if thisDepth > maxDepth: maxDepth = thisDepth
    return maxDepth
```

**firstStr = myTree.keys()[0]是什么语法？**
这是 Python 中字典数据类型的方法之一，用于获取字典的第一个键值对的键。在这里，myTree是一个字典，keys()方法返回字典中的所有键，使用 [0] 取得第一个键。由于字典是无序的，因此不能保证每次执行 keys() 方法都会得到相同的键值对顺序。



### 3、朴素贝叶斯

使用朴素贝叶斯进行文档分类

```python
def loadDataSet():
    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not
    return postingList,classVec
                 
def createVocabList(dataSet):
    vocabSet = set([])  #create empty set
    for document in dataSet:
        vocabSet = vocabSet | set(document) #union of the two sets
    return list(vocabSet)

def setOfWords2Vec(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else: print "the word: %s is not in my Vocabulary!" % word
    return returnVec
#计算条件概率
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() 
    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = log(p1Num/p1Denom)          #change to log()
    p0Vect = log(p0Num/p0Denom)          #change to log()
    return p0Vect,p1Vect,pAbusive
```

#### 应用：朴素贝叶斯对电子邮件进行的分类

```python
def spamTest():

  docList=[]; classList = []; fullText =[]

  for i in range(1,26):

​    wordList = textParse(open('email/spam/%d.txt' % i).read())

​    docList.append(wordList)

​    fullText.extend(wordList)

​    classList.append(1)

​    wordList = textParse(open('email/ham/%d.txt' % i).read())

​    docList.append(wordList)

​    fullText.extend(wordList)

​    classList.append(0)

  vocabList = createVocabList(docList)#create vocabulary

  trainingSet = range(50); testSet=[]      #create test set

  for i in range(10):

​    randIndex = int(random.uniform(0,len(trainingSet)))

​    testSet.append(trainingSet[randIndex])

​    del(trainingSet[randIndex])  

  trainMat=[]; trainClasses = []

  for docIndex in trainingSet:#train the classifier (get probs) trainNB0

​    trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))

​    trainClasses.append(classList[docIndex])

  p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))

  errorCount = 0

  for docIndex in testSet:     #classify the remaining items

​    wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])

​    if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:

​      errorCount += 1

​      print "classification error",docList[docIndex]

  print 'the error rate is: ',float(errorCount)/len(testSet)

  \#return vocabList,fullText
```

这是一个朴素贝叶斯垃圾邮件分类器的实现。函数中的流程大致如下：

遍历邮件数据集中的每个邮件，分别读取垃圾邮件和正常邮件，并使用 textParse() 函数将邮件内容解析成单词列表。
创建空的 docList，classList 和 fullText 列表，用于存储单词列表、邮件分类和所有单词的列表。
将解析后的单词列表加入 docList，将所有单词加入 fullText，并分别将邮件分类 1 和 0 加入 classList，表示垃圾邮件和正常邮件。
调用 createVocabList() 函数创建词汇表 vocabList。
创建训练集 trainingSet 和测试集 testSet，从所有邮件中随机选取10个邮件加入测试集中，其余的邮件加入训练集中。
对于训练集中的每个邮件，使用 bagOfWords2VecMN() 函数将其转化为词向量，并将其加入 trainMat 中，同时将其分类加入 trainClasses 中。
调用 trainNB0() 函数训练朴素贝叶斯分类器，得到分类器的概率分布 p0V、p1V 和 pSpam。
对于测试集中的每个邮件，使用 bagOfWords2VecMN() 函数将其转化为词向量，然后使用 classifyNB() 函数对其进行分类，并将分类结果与实际分类进行比较，统计分类错误的个数。
最后输出分类错误率。

其中语法：

**trainingSet = range(50);**

这一行代码将训练集索引初始化为0到49的整数列表。在这个具体的例子中，数据集中共有50个文档，前25个是垃圾邮件，后25个是非垃圾邮件。因此，这一行代码将训练集初始化为数据集中所有文档的索引，共50个。在之后的代码中，会从训练集中随机选择一些索引作为测试集。

**int(random.uniform(0,len(trainingSet)))**

这行代码的作用是从 `trainingSet` 中随机选取一个整数作为索引值，用于从数据集中选择一个文档作为测试集。`random.uniform(0,len(trainingSet))` 返回一个从 0 到 `len(trainingSet)` 的随机浮点数，`int()` 将其转换为整数。因为 `int()` 向下取整，所以返回的整数总是小于 `len(trainingSet)` 的。

**array(trainMat),array(trainClasses) array是什么语法？**

`array`是Python标准库`numpy`中的一个函数，用于将输入的序列转换为`ndarray`（即多维数组）类型。在这里，`trainMat`和`trainClasses`是输入序列，将它们转换成`ndarray`类型的数据可以更高效地进行数值计算。

### 4、logistics 回归

#### 使用梯度上升寻找最佳参数

```python
def gradAscent(dataMatIn, classLabels):

  dataMatrix = mat(dataMatIn)       #convert to NumPy matrix

  labelMat = mat(classLabels).transpose() #convert to NumPy matrix

  m,n = shape(dataMatrix)

  alpha = 0.001

  maxCycles = 500

  weights = ones((n,1))

  for k in range(maxCycles):        #heavy on matrix operations

​    h = sigmoid(dataMatrix*weights)   #matrix mult

​    error = (labelMat - h)        #vector subtraction

​    weights = weights + alpha * dataMatrix.transpose()* error #matrix mult

  return weights
```

求出权重后，画出拟合的函数

```python
def plotBestFit(weights):

  import matplotlib.pyplot as plt

  dataMat,labelMat=loadDataSet()

  dataArr = array(dataMat)

  n = shape(dataArr)[0] 

  xcord1 = []; ycord1 = []

  xcord2 = []; ycord2 = []

  for i in range(n):

​    if int(labelMat[i])== 1:

​      xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])

​    else:

​      xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])

  fig = plt.figure()

  ax = fig.add_subplot(111)

  ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')

  ax.scatter(xcord2, ycord2, s=30, c='green')

  x = arange(-3.0, 3.0, 0.1)

  y = (-weights[0]-weights[1]*x)/weights[2]

  ax.plot(x, y)

  plt.xlabel('X1'); plt.ylabel('X2');

  plt.show()
```

#### 随机梯度上升算法

```python
def stocGradAscent1(dataMatrix, classLabels, numIter=150):
    m,n = shape(dataMatrix)
    weights = ones(n)   #initialize to all ones
    for j in range(numIter):
        dataIndex = range(m)
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.0001    #apha decreases with iteration, does not 
            randIndex = int(random.uniform(0,len(dataIndex)))#go to 0 because of the constant
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha * error * dataMatrix[randIndex]
            del(dataIndex[randIndex])
    return weights
```

### 5、支持向量机

svm相对于上面的逻辑回归，更在乎的是分割是否更加健壮。

理论部分：

![image-20230308143211474](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230308143211474.png)

约束条件：

![image-20230308143239601](C:\Users\wfp\AppData\Roaming\Typora\typora-user-images\image-20230308143239601.png)

SVM（Support Vector Machines）是一种非常强大的分类算法，其中Smo（Sequential Minimal Optimization）是SVM中一种常用的优化算法。

Smo算法的主要思路是将大优化问题拆分成多个小优化问题来解决。它采用一种叫做“块”的方法，每次选择两个变量优化来最大化目标函数，并固定其他变量。

具体而言，Smo算法的主要步骤如下：

初始化：选择两个变量作为第一次优化的变量，并初始化所有变量的参数。

选择变量：使用一定的策略选择两个变量，其中一个是违反KKT条件的变量，另一个是使得目标函数增加最快的变量。

优化变量：固定其他变量，优化选定的两个变量，使目标函数增加。

更新参数：更新所有变量的参数，包括所选的两个变量。

循环执行步骤2-4，直到达到终止条件（例如，达到最大迭代次数或满足一定的收敛条件）。

通过这种分块的方式，Smo算法可以高效地解决大规模的优化问题，并在实践中被广泛应用于SVM的训练。

#### 简化版SMO算法处理小规模数据集

```python
def smoSimple(dataMatIn, classLabels, C, toler, maxIter):

  dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()

  b = 0; m,n = shape(dataMatrix)

  alphas = mat(zeros((m,1)))

  iter = 0

  while (iter < maxIter):

​    alphaPairsChanged = 0

​    for i in range(m):

​      fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b

​      Ei = fXi - float(labelMat[i])#if checks if an example violates KKT conditions

​      if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):

​        j = selectJrand(i,m)

​        fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b

​        Ej = fXj - float(labelMat[j])

​        alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();

​        if (labelMat[i] != labelMat[j]):

​          L = max(0, alphas[j] - alphas[i])

​          H = min(C, C + alphas[j] - alphas[i])

​        else:

​          L = max(0, alphas[j] + alphas[i] - C)

​          H = min(C, alphas[j] + alphas[i])

​        if L==H: print "L==H"; continue

​        eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T

​        if eta >= 0: print "eta>=0"; continue

​        alphas[j] -= labelMat[j]*(Ei - Ej)/eta

​        alphas[j] = clipAlpha(alphas[j],H,L)

​        if (abs(alphas[j] - alphaJold) < 0.00001): print "j not moving enough"; continue

​        alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#update i by the same amount as j

​                                    \#the update is in the oppostie direction

​        b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T

​        b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T

​        if (0 < alphas[i]) and (C > alphas[i]): b = b1

​        elif (0 < alphas[j]) and (C > alphas[j]): b = b2

​        else: b = (b1 + b2)/2.0

​        alphaPairsChanged += 1

​        print "iter: %d i:%d, pairs changed %d" % (iter,i,alphaPairsChanged)

​    if (alphaPairsChanged == 0): iter += 1

​    else: iter = 0

​    print "iteration number: %d" % iter

  return b,alphas
```

**if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):**

这是SVM中SMO算法中的一个重要的判断条件，用于选择优化的变量。

其中，labelMat[i]表示训练数据集中第i个样本的类别标签，Ei表示该样本当前预测值与真实值之间的差距（也称为误差），alphas[i]表示该样本对应的拉格朗日乘子。

toler是一个用户定义的容错率，C是SVM中的正则化参数，用于平衡模型的分类性能和泛化能力。

这个判断条件的意思是，如果当前的样本i违反了KKT条件，即其预测值与真实值之间的差距超过了toler容错率，并且其对应的拉格朗日乘子alphas[i]没有达到其上界C（如果该样本为正样本）或下界0（如果该样本为负样本），那么这个样本就是一个可以被优化的样本。

换句话说，这个判断条件是用来选择满足以下条件的变量：1）在间隔边界上的支持向量；2）在间隔边界和分类边界之间的样本点。对这些变量进行优化可以使得目标函数值增大，并最终收敛于最优解。

**eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T**

在SMO算法中，eta是计算变量更新量的一个重要值。其具体计算方法如下：

假设要更新的两个变量的下标分别为i和j，数据集的特征矩阵为dataMatrix，那么eta的计算方式为：

eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T

其中，dataMatrix[i,:]表示数据集中第i个样本的特征向量，dataMatrix[j,:]表示数据集中第j个样本的特征向量，T表示矩阵的转置。

这个式子的意义是，通过计算两个变量对应的样本之间的内积和它们各自的内积，得到它们更新量的大小。

如果eta为负数，则说明优化的目标函数不能被优化，此时需要退出当前循环；如果eta为非负数，则可以继续优化，选择最大的更新量进行优化。

**if (abs(alphas[j] - alphaJold) < 0.00001): print "j not moving enough"; continue**

这段代码是用来判断拉格朗日乘子alpha[j]是否收敛的。在SMO算法中，当优化变量alpha[j]的值发生变化时，需要判断其变化是否足够大，如果不够大，则可以认为该变量已经收敛了，可以跳过后面的计算。

具体地，这段代码中，`alphaJold`表示变量alpha[j]在更新之前的值，`abs(alphas[j] - alphaJold)`用来计算alpha[j]更新后的变化量。如果变化量小于一个设定的阈值`0.00001`，就认为该变量已经收敛，可以跳过后面的计算，并输出一条提示信息`"j not moving enough"`。

这个判断条件的意义是：如果alpha[j]已经收敛，则不需要再继续进行更新；如果alpha[j]未收敛，则需要继续进行更新，直到达到收敛条件。

使用`continue`语句可以跳过后面的计算，直接进入下一次循环，以提高程序的效率。同时，输出提示信息也有助于我们了解算法的执行过程，及时发现可能存在的问题。

### 6、adaboost算法

训练一次之后，重点关注那些训练出错的样本，提高他们的权值。

首先构建了一个stumpclassify的分类方法，并通过构建树桩得到错误率最低的分类器。

```python
def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#just classify the data
    retArray = ones((shape(dataMatrix)[0],1))
    if threshIneq == 'lt':
        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0
    else:
        retArray[dataMatrix[:,dimen] > threshVal] = -1.0
    return retArray
    

def buildStump(dataArr,classLabels,D):
    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T
    m,n = shape(dataMatrix)
    numSteps = 10.0; bestStump = {}; bestClasEst = mat(zeros((m,1)))
    minError = inf #init error sum, to +infinity
    for i in range(n):#loop over all dimensions
        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();
        stepSize = (rangeMax-rangeMin)/numSteps
        for j in range(-1,int(numSteps)+1):#loop over all range in current dimension
            for inequal in ['lt', 'gt']: #go over less than and greater than
                threshVal = (rangeMin + float(j) * stepSize)
                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)#call stump classify with i, j, lessThan
                errArr = mat(ones((m,1)))
                errArr[predictedVals == labelMat] = 0
                print "predictedVals",predictedVals.T,"errArr",errArr.T
                weightedError = D.T*errArr  #calc total error multiplied by D
                print "split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError)
                if weightedError < minError:
                    minError = weightedError
                    bestClasEst = predictedVals.copy()
                    bestStump['dim'] = i
                    bestStump['thresh'] = threshVal
                    bestStump['ineq'] = inequal
    return bestStump,minError,bestClasEst

```

adaboost算法实现

```python
def adaBoostTrainDS(dataArr,classLabels,numIt=40):
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat(ones((m,1))/m)   #init D to all equal
    aggClassEst = mat(zeros((m,1)))
    for i in range(numIt):
        bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump
        #print "D:",D.T
        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0
        bestStump['alpha'] = alpha  
        weakClassArr.append(bestStump)                  #store Stump Params in Array
        #print "classEst: ",classEst.T
        expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy
        D = multiply(D,exp(expon))                              #Calc New D for next iteration
        D = D/D.sum()
        #calc training error of all classifiers, if this is 0 quit for loop early (use break)
        aggClassEst += alpha*classEst
        #print "aggClassEst: ",aggClassEst.T
        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))
        errorRate = aggErrors.sum()/m
        print "total error: ",errorRate
        if errorRate == 0.0: break
    return weakClassArr,aggClassEst
```

adaboost算法实现

```python
def adaBoostTrainDS(dataArr,classLabels,numIt=40):
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat(ones((m,1))/m)   #init D to all equal
    aggClassEst = mat(zeros((m,1)))
    for i in range(numIt):
        bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump
        #print "D:",D.T
        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0
        bestStump['alpha'] = alpha  
        weakClassArr.append(bestStump)                  #store Stump Params in Array
        #print "classEst: ",classEst.T
        expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy
        D = multiply(D,exp(expon))                              #Calc New D for next iteration
        D = D/D.sum()
        #calc training error of all classifiers, if this is 0 quit for loop early (use break)
        aggClassEst += alpha*classEst
        #print "aggClassEst: ",aggClassEst.T
        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))
        errorRate = aggErrors.sum()/m
        print "total error: ",errorRate
        if errorRate == 0.0: break
    return weakClassArr,aggClassEst
```

**numpy中multiply的用法**

`numpy.multiply()`是NumPy中的一个函数，用于执行数组的逐元素乘法（element-wise multiplication），即将两个数组中对应位置的元素相乘。它的语法如下：

```python
pythonCopy code
numpy.multiply(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])
```

其中，`x1`和`x2`是要相乘的两个数组，可以是NumPy数组、Python列表、元组或标量值。`out`是可选参数，用于指定结果的输出位置。`where`是可选参数，用于指定只有满足条件的元素才进行相乘操作。其他参数用于控制数据类型、内存布局等细节。

`numpy.multiply()`函数的返回值是一个新的数组，它与`x1`和`x2`具有相同的形状和数据类型。例如，如果`x1`和`x2`都是形状为`(m,n)`的NumPy数组，则`numpy.multiply(x1, x2)`将返回一个形状为`(m,n)`的新数组，其中第`i,j`个元素的值为`x1[i,j] * x2[i,j]`。

`numpy.multiply()`函数可以与其他NumPy函数组合使用，例如用于计算向量点积、矩阵乘法等。它也可以用于执行逐元素的标量运算，例如将数组中的每个元素乘以一个标量。

## 第二部分：利用回归预测数值型数据

### 7、线性回归

在线性回归中，我们尝试找到一条直线（或者是更高维度的超平面），使得这条直线能够最好地拟合已知的数据点。具体来说，我们可以采用最小二乘法（Least Squares Method）来找到最佳拟合曲线。

最小二乘法是一种常见的线性回归方法，其基本思想是最小化预测值与真实值之间的平方误差。具体地，对于一个数据集$D={(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)}$，其中$x_i$表示第$i$个样本的特征向量，$y_i$表示对应的真实标签，我们尝试找到一个线性函数$h(x)$，使得$h(x)$能够最小化下式：
$$
\sum_{i=1}^n(h(x_i)-y_i)^2
$$


我们将上式称为目标函数，也就是要最小化的函数。为了找到最小化目标函数的$h(x)$，我们需要求解其参数，即权重$w$和偏置$b$。一般来说，我们可以使用梯度下降法或者矩阵求导法求解。

具体地，对于梯度下降法，我们需要初始化权重$w$和偏置$b$，然后不断迭代地更新它们，直到目标函数的值不再变化或者达到一定的迭代次数为止。每次迭代时，我们需要计算目标函数对$w$和$b$的偏导数，然后按照梯度的反方向更新$w$和$b$，即：
$$
w_j \leftarrow w_j - \alpha\frac{\partial}{\partial w_j}\sum_{i=1}^n(h(x_i)-y_i)^2
$$

$$
b \leftarrow b - \alpha\frac{\partial}{\partial b}\sum_{i=1}^n(h(x_i)-y_i)^2
$$

其中，$\alpha$表示学习率，控制着每次更新的步长。具体地，我们可以采用以下公式计算偏导数：
$$
\frac{\partial}{\partial w_j}\sum_{i=1}^n(h(x_i)-y_i)^2 = 2\sum_{i=1}^n(h(x_i)-y_i)x_{ij}
$$

$$
\frac{\partial}{\partial b}\sum_{i=1}^n(h(x_i)-y_i)^2 = 2\sum_{i=1}^n(h(x_i)-y_i)
$$
这里，$x_{ij}$表示第$i$个样本的第$j$个特征值。

对于矩阵求导法，我们可以直接对目标函数进行求导，得到其最优解的闭式解析解。具体来说，我们可以将目标函数表示为向量形式，即：
$$
J(w,b) = (Xw-y)^T(Xw-y)
$$

```python
def standRegres(xArr,yArr):
    xMat = mat(xArr); yMat = mat(yArr).T
    xTx = xMat.T*xMat
    if linalg.det(xTx) == 0.0:
        print "This matrix is singular, cannot do inverse"
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws
```

#### 局部加权回归

线性回归如用出现过拟合的现象，于是将更接近的点赋予相应的权值。

```python
def lwlr(testPoint,xArr,yArr,k=1.0):
    xMat = mat(xArr); yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye((m)))
    for j in range(m):                      #next 2 lines create weights matrix
        diffMat = testPoint - xMat[j,:]     #
        weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))
    xTx = xMat.T * (weights * xMat)
    if linalg.det(xTx) == 0.0:
        print "This matrix is singular, cannot do inverse"
        return
    ws = xTx.I * (xMat.T * (weights * yMat))
    return testPoint * ws

def lwlrTest(testArr,xArr,yArr,k=1.0):  #loops over all the data points and applies lwlr to each one
    m = shape(testArr)[0]
    yHat = zeros(m)
    for i in range(m):
        yHat[i] = lwlr(testArr[i],xArr,yArr,k)
    return yHat
```

### 8、树回归

树回归是一种基于树结构的回归模型，它通过对训练集的递归划分来构建回归模型。与分类树不同，树回归的目标是预测一个连续值而不是离散的类别标签。

在树回归中，每个节点代表一个特征，每个分支代表该特征的一个取值，而每个叶子节点则代表一个预测值。构建树回归模型的过程中，通常采用贪心算法，即每次选择最优的特征和取值进行分裂，直到达到某个预定的停止条件，如树的深度、叶子节点的样本数等。

常用的树回归算法包括CART算法和C4.5算法。CART算法通过最小化平方误差来选择最佳的特征和取值进行分裂；而C4.5算法则通过最大化信息增益比来选择最佳的特征和取值进行分裂。

树回归的优点在于：

1. 可解释性强：树回归生成的模型可以可视化，易于理解和解释，有助于发现数据中的模式和规律；
2. 鲁棒性好：树回归对异常值和噪声数据不敏感；
3. 可扩展性强：树回归可用于处理各种类型的数据，包括数值型和分类型数据；
4. 计算复杂度低：树回归的训练和预测过程都非常快速，适用于大规模数据集。

树回归的缺点在于：

1. 容易过拟合：树回归在处理复杂数据集时容易过拟合，需要使用剪枝技术或集成学习等方法来避免过拟合；
2. 不稳定性强：树回归对于输入数据的微小变化非常敏感，可能导致不稳定的预测结果；
3. 难以处理缺失值：树回归对于缺失值的处理能力较弱，需要使用特殊的方法来处理缺失值。

```python
def loadDataSet(fileName):      #general function to parse tab -delimited floats
    dataMat = []                #assume last column is target value
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = map(float,curLine) #map all elements to float()
        dataMat.append(fltLine)
    return dataMat

def binSplitDataSet(dataSet, feature, value):
    mat0 = dataSet[nonzero(dataSet[:,feature] > value)[0],:][0]
    mat1 = dataSet[nonzero(dataSet[:,feature] <= value)[0],:][0]
    return mat0,mat1
```

CART算法实现

```python
def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):#assume dataSet is NumPy Mat so we can array filtering
    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)#choose the best split
    if feat == None: return val #if the splitting hit a stop condition return val
    retTree = {}
    retTree['spInd'] = feat
    retTree['spVal'] = val
    lSet, rSet = binSplitDataSet(dataSet, feat, val)
    retTree['left'] = createTree(lSet, leafType, errType, ops)
    retTree['right'] = createTree(rSet, leafType, errType, ops)
    return retTree  

```

如何选择最佳的方式进行切割呢？

```python
def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):
    tolS = ops[0]; tolN = ops[1]
    #if all the target variables are the same value: quit and return value
    if len(set(dataSet[:,-1].T.tolist()[0])) == 1: #exit cond 1
        return None, leafType(dataSet)
    m,n = shape(dataSet)
    #the choice of the best feature is driven by Reduction in RSS error from mean
    S = errType(dataSet)
    bestS = inf; bestIndex = 0; bestValue = 0
    for featIndex in range(n-1):
        for splitVal in set(dataSet[:,featIndex]):
            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)
            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): continue
            newS = errType(mat0) + errType(mat1)
            if newS < bestS: 
                bestIndex = featIndex
                bestValue = splitVal
                bestS = newS
    #if the decrease (S-bestS) is less than a threshold don't do the split
    if (S - bestS) < tolS: 
        return None, leafType(dataSet) #exit cond 2
    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)
    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):  #exit cond 3
        return None, leafType(dataSet)
    return bestIndex,bestValue#returns the best feature to split on
                              #and the value used for that split
```

这段代码是用来寻找最佳特征和最佳切分点的函数。函数名称为 chooseBestSplit，接受四个参数：

dataSet：输入的数据集，是一个二维的NumPy数组，每一行代表一个样本，最后一列为样本的标签或输出。

leafType：叶节点生成函数，默认为 regLeaf，即用线性回归拟合生成叶节点。

errType：误差计算函数，默认为 regErr，即用总方差计算误差。

ops：包含两个参数的元组，用于控制算法停止时的条件。tolS 为误差的最小降低值，tolN 为切分的最小样本数。

函数使用了嵌套循环，对每个特征和每个可能的切分点进行尝试，并计算误差。在计算误差时，先将数据集按照当前特征和切分点进行二元切分，然后计算切分后的两个数据集的误差和。最终选择使得误差降低最多的特征和切分点作为最佳的切分方法。

函数的返回值是一个元组 (bestIndex, bestValue)，表示最佳切分点的特征索引和最佳切分点的取值。如果无法再分或者误差降低不足以分裂数据，则返回 None 和叶节点的值。

#### 树剪枝

预剪枝、后剪枝--先不看了吧

## 第三部分：无监督学习

无监督学习是没有训练过程的

### 9、利用K均值聚类算法对未标注数据分组（K-means）



```python
def distEclud(vecA, vecB):
    return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)

def randCent(dataSet, k):
    n = shape(dataSet)[1]
    centroids = mat(zeros((k,n)))#create centroid mat
    for j in range(n):#create random cluster centers, within bounds of each dimension
        minJ = min(dataSet[:,j]) 
        rangeJ = float(max(dataSet[:,j]) - minJ)
        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1))
    return centroids
    
def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m,2)))#create mat to assign data points 
                                      #to a centroid, also holds SE of each point
    centroids = createCent(dataSet, k)
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        for i in range(m):#for each data point assign it to the closest centroid
            minDist = inf; minIndex = -1
            for j in range(k):
                distJI = distMeas(centroids[j,:],dataSet[i,:])
                if distJI < minDist:
                    minDist = distJI; minIndex = j
            if clusterAssment[i,0] != minIndex: clusterChanged = True
            clusterAssment[i,:] = minIndex,minDist**2
        print centroids
        for cent in range(k):#recalculate centroids
            ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#get all the point in this cluster
            centroids[cent,:] = mean(ptsInClust, axis=0) #assign centroid to mean 
    return centroids, clusterAssment

```

二分K-均值聚类算法

```python
def biKmeans(dataSet, k, distMeas=distEclud):
    m = shape(dataSet)[0]
    clusterAssment = mat(zeros((m,2)))
    centroid0 = mean(dataSet, axis=0).tolist()[0]
    centList =[centroid0] #create a list with one centroid
    for j in range(m):#calc initial Error
        clusterAssment[j,1] = distMeas(mat(centroid0), dataSet[j,:])**2
    while (len(centList) < k):
        lowestSSE = inf
        for i in range(len(centList)):
            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:]#get the data points currently in cluster i
            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)
            sseSplit = sum(splitClustAss[:,1])#compare the SSE to the currrent minimum
            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A!=i)[0],1])
            print "sseSplit, and notSplit: ",sseSplit,sseNotSplit
            if (sseSplit + sseNotSplit) < lowestSSE:
                bestCentToSplit = i
                bestNewCents = centroidMat
                bestClustAss = splitClustAss.copy()
                lowestSSE = sseSplit + sseNotSplit
        bestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList) #change 1 to 3,4, or whatever
        bestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit
        print 'the bestCentToSplit is: ',bestCentToSplit
        print 'the len of bestClustAss is: ', len(bestClustAss)
        centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0]#replace a centroid with two best centroids 
        centList.append(bestNewCents[1,:].tolist()[0])
        clusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss#reassign new clusters, and SSE
    return mat(centList), clusterAssment
```

### 10、使用Apriori算法进行关联分析

```python
def createC1(dataSet):
    C1 = []
    for transaction in dataSet:
        for item in transaction:
            if not [item] in C1:
                C1.append([item])
                
    C1.sort()
    return map(frozenset, C1)#use frozen set so we
                            #can use it as a key in a dict    

def scanD(D, Ck, minSupport):
    ssCnt = {}
    for tid in D:
        for can in Ck:
            if can.issubset(tid):
                if not ssCnt.has_key(can): ssCnt[can]=1
                else: ssCnt[can] += 1
    numItems = float(len(D))
    retList = []
    supportData = {}
    for key in ssCnt:
        support = ssCnt[key]/numItems
        if support >= minSupport:
            retList.insert(0,key)
        supportData[key] = support
    return retList, supportData

```

**map(frozenset, C1)什么意思？**

这段代码涉及到Python中的两个函数，map()和frozenset()，以及变量C1。

首先，frozenset()是Python中的一个内置函数，它可以将一个可迭代对象转化为一个不可变的集合，即冻结集合。与普通的集合(set)不同，冻结集合可以作为其他集合的元素，因为它是不可变的。

其次，map()是Python内置的高阶函数，它接受两个参数：一个是函数，一个是可迭代对象，它会对可迭代对象中的每个元素应用函数，并返回一个新的可迭代对象，其中每个元素都是原可迭代对象中对应元素经过函数处理后的结果。

因此，map(frozenset, C1)的意思是将变量C1中的每个元素都转化为一个冻结集合，并将所有冻结集合作为一个新的可迭代对象返回。换句话说，这行代码将C1中的每个集合转化为冻结集合，并返回一个由这些冻结集合组成的新集合。

**举个例子看一下**

好的，我们可以通过一个简单的例子来说明map(frozenset, C1)的作用。

假设变量C1是一个集合列表，其中包含了两个集合：

```python
pythonCopy code
C1 = [{1, 2, 3}, {2, 3, 4}]
```

现在，我们可以使用map(frozenset, C1)将C1中的每个集合转化为一个冻结集合，即：

```python
pythonCopy code
frozen_C1 = map(frozenset, C1)
```

得到的frozen_C1是一个由两个冻结集合组成的新列表：

```python
pythonCopy code
[frozenset({1, 2, 3}), frozenset({2, 3, 4})]
```

这个新列表中的每个元素都是一个不可变的集合，因此我们可以安全地将它们用作其他集合的元素，比如：

```python
pythonCopy code
C2 = {frozenset({1, 2, 3}), frozenset({2, 3, 4})}
```

得到的C2是一个由两个冻结集合组成的集合，其中每个元素都是不可变的。这样的集合可以被用作字典的键，或者作为其他集合的元素，因为它们不可变，不会出现因为改变元素导致集合无法正常使用的问题。

apriori算法：

```python
def aprioriGen(Lk, k): #creates Ck
    retList = []
    lenLk = len(Lk)
    for i in range(lenLk):
        for j in range(i+1, lenLk): 
            L1 = list(Lk[i])[:k-2]; L2 = list(Lk[j])[:k-2]
            L1.sort(); L2.sort()
            if L1==L2: #if first k-2 elements are equal
                retList.append(Lk[i] | Lk[j]) #set union
    return retList

def apriori(dataSet, minSupport = 0.5):
    C1 = createC1(dataSet)
    D = map(set, dataSet)
    L1, supportData = scanD(D, C1, minSupport)
    L = [L1]
    k = 2
    while (len(L[k-2]) > 0):
        Ck = aprioriGen(L[k-2], k)
        Lk, supK = scanD(D, Ck, minSupport)#scan DB to get Lk
        supportData.update(supK)
        L.append(Lk)
        k += 1
    return L, supportData
```

### 11、使用FP-growth算法来搞笑发现频繁项集

fp树的定义：

```python
class treeNode:
    def __init__(self, nameValue, numOccur, parentNode):
        self.name = nameValue
        self.count = numOccur
        self.nodeLink = None
        self.parent = parentNode      #needs to be updated
        self.children = {} 
    
    def inc(self, numOccur):
        self.count += numOccur
        
    def disp(self, ind=1):
        print '  '*ind, self.name, ' ', self.count
        for child in self.children.values():
            child.disp(ind+1)
```

这是一个Python类，用于创建FP树（频繁模式树）的节点。FP树是一种数据结构，用于存储在交易数据集中频繁出现的项集。该类具有以下属性和方法：

- name：节点的名称（即项的名称）。
- count：项集在数据集中出现的次数。
- nodeLink：指向相同项名称的下一个节点的指针（用于链接相同的项）。
- parent：指向父节点的指针。
- children：一个包含子节点的字典，其中键是子节点的名称，值是子节点对象。

__init__方法是类的构造函数，它定义了新节点的名称，计数和父节点，并初始化了nodeLink和children属性。

inc方法用于增加节点的计数值，以便在FP树中处理项集时更新计数。

disp方法用于打印FP树的节点和子节点，以便调试。参数ind用于控制打印格式的缩进程度。

```python
def createTree(dataSet, minSup=1): #create FP-tree from dataset but don't mine
    headerTable = {}
    #go over dataSet twice
    for trans in dataSet:#first pass counts frequency of occurance
        for item in trans:
            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]
    for k in headerTable.keys():  #remove items not meeting minSup
        if headerTable[k] < minSup: 
            del(headerTable[k])
    freqItemSet = set(headerTable.keys())
    #print 'freqItemSet: ',freqItemSet
    if len(freqItemSet) == 0: return None, None  #if no items meet min support -->get out
    for k in headerTable:
        headerTable[k] = [headerTable[k], None] #reformat headerTable to use Node link 
    #print 'headerTable: ',headerTable
    retTree = treeNode('Null Set', 1, None) #create tree
    for tranSet, count in dataSet.items():  #go through dataset 2nd time
        localD = {}
        for item in tranSet:  #put transaction items in order
            if item in freqItemSet:
                localD[item] = headerTable[item][0]
        if len(localD) > 0:
            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]
            updateTree(orderedItems, retTree, headerTable, count)#populate tree with ordered freq itemset
    return retTree, headerTable #return tree and header table

def updateTree(items, inTree, headerTable, count):
    if items[0] in inTree.children:#check if orderedItems[0] in retTree.children
        inTree.children[items[0]].inc(count) #incrament count
    else:   #add items[0] to inTree.children
        inTree.children[items[0]] = treeNode(items[0], count, inTree)
        if headerTable[items[0]][1] == None: #update header table 
            headerTable[items[0]][1] = inTree.children[items[0]]
        else:
            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])
    if len(items) > 1:#call updateTree() with remaining ordered items
        updateTree(items[1::], inTree.children[items[0]], headerTable, count)
        
```

## 机器学习八股
